# -*- coding: utf-8 -*-
import io
from io import BytesIO
# TsFreshæ—¶é—´åºåˆ—çš„ç‰¹å¾å·¥ç¨‹ï¼ŒåŒ…æ‹¬ç‰¹å¾ç”Ÿæˆå’Œç‰¹å¾é€‰æ‹©
import os
import gc
import time
import logging
import zipfile

import numpy as np
import pandas as pd
from pandas.errors import ParserError
from numpy import vstack, array, nan
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st

from scipy.stats import pearsonr
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster

from itertools import combinations

from tsfresh import extract_features
from tsfresh import select_features
from tsfresh.utilities.dataframe_functions import impute

from sklearn.cluster import KMeans
from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, Binarizer
from sklearn.impute import SimpleImputer  # å¤„ç†ç¼ºå¤±å€¼çš„ç±»
from sklearn.inspection import permutation_importance  # ç”¨äºè®¡ç®—ç‰¹å¾çš„æ’åˆ—é‡è¦æ€§
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE, SelectFromModel
from sklearn.datasets import load_iris
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import Lasso, LassoCV, LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
# import eli5  # è§£é‡Šæœºå™¨å­¦ä¹ æ¨¡å‹å’Œé¢„æµ‹ç»“æœçš„åº“
# from eli5.sklearn import PermutationImportance
import platform
import warnings

warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="StreamlitFeatureEngineer",
    page_icon="ğŸŒŸ",
    layout="wide",  # "centered" æˆ– "wide"ï¼Œå®½å¸ƒå±€æ›´ç°ä»£
    initial_sidebar_state="expanded"  # ä¾§è¾¹æ é»˜è®¤å±•å¼€
)
st.title('ç‰¹å¾å·¥ç¨‹å®ç°æµç¨‹')

# system = platform.system()
# if system == 'Windows':
#     # Windows
#     plt.rcParams['font.sans-serif'] = ['SimHei']
# elif system == 'Darwin':
#     # MacOS
#     plt.rcParams['font.sans-serif'] = 'Songti Sc'

# plt.rcParams['font.sans-serif'] = ['SimHei']
# plt.rcParams['font.sans-serif'] = 'Songti Sc'
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®loggingæ¨¡å—
logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFO
    format='%(asctime)s - %(levelname)s - %(message)s',  # æ—¥å¿—æ ¼å¼
    handlers=[
        logging.FileHandler('app.log')  # æ—¥å¿—ä¿å­˜åˆ°app.logæ–‡ä»¶ä¸­
        # logging.StreamHandler()  # åŒæ—¶åœ¨æ§åˆ¶å°è¾“å‡ºæ—¥å¿—
    ]
)


class LR(LogisticRegression):
    def __init__(self,
                 threshold=0.01,
                 dual=False,
                 tol=1e-4,
                 C=1.0,
                 fit_intercept=True,
                 intercept_scaling=1,
                 class_weight=None,
                 random_state=None,
                 solver='liblinear',
                 max_iter=100,
                 multi_class='ovr',
                 verbose=0,
                 warm_start=False,
                 n_jobs=1):
        # æƒå€¼ç›¸è¿‘çš„é˜ˆå€¼
        self.threshold = threshold
        LogisticRegression.__init__(self,
                                    penalty='l1',
                                    dual=dual,
                                    tol=tol,
                                    C=C,
                                    fit_intercept=fit_intercept,
                                    intercept_scaling=intercept_scaling,
                                    class_weight=class_weight,
                                    random_state=random_state,
                                    solver=solver,
                                    max_iter=max_iter,
                                    multi_class=multi_class,
                                    verbose=verbose,
                                    warm_start=warm_start,
                                    n_jobs=n_jobs)
        # ä½¿ç”¨åŒæ ·çš„å‚æ•°åˆ›å»ºL2é€»è¾‘å›å½’
        self.l2 = LogisticRegression(penalty='l2', dual=dual, tol=tol, C=C, fit_intercept=fit_intercept,
                                     intercept_scaling=intercept_scaling, class_weight=class_weight,
                                     random_state=random_state, solver=solver, max_iter=max_iter,
                                     multi_class=multi_class, verbose=verbose, warm_start=warm_start, n_jobs=n_jobs)

    def fit(self, X, y, sample_weight=None):
        # è®­ç»ƒL1é€»è¾‘å›å½’
        super(LR, self).fit(X, y, sample_weight=sample_weight)
        self.coef_old_ = self.coef_.copy()
        # è®­ç»ƒL2é€»è¾‘å›å½’
        self.l2.fit(X, y, sample_weight=sample_weight)
        cntOfRow, cntOfCol = self.coef_.shape
        # æƒå€¼ç³»æ•°çŸ©é˜µçš„è¡Œæ•°å¯¹åº”ç›®æ ‡å€¼çš„ç§ç±»æ•°ç›®
        for i in range(cntOfRow):
            for j in range(cntOfCol):
                coef = self.coef_[i][j]
                # L1é€»è¾‘å›å½’çš„æƒå€¼ç³»æ•°ä¸ä¸º0
                if coef != 0:
                    idx = [j]
                    # å¯¹åº”åœ¨L2é€»è¾‘å›å½’ä¸­çš„æƒå€¼ç³»æ•°
                    coef1 = self.l2.coef_[i][j]
                    for k in range(cntOfCol):
                        coef2 = self.l2.coef_[i][k]
                        # åœ¨L2é€»è¾‘å›å½’ä¸­ï¼Œæƒå€¼ç³»æ•°ä¹‹å·®å°äºè®¾å®šçš„é˜ˆå€¼ï¼Œä¸”åœ¨L1ä¸­å¯¹åº”çš„æƒå€¼ä¸º0
                        if abs(coef1 - coef2) < self.threshold and j != k and self.coef_[i][k] == 0:
                            idx.append(k)
                    # è®¡ç®—è¿™ä¸€ç±»ç‰¹å¾çš„æƒå€¼ç³»æ•°å‡å€¼
                    mean = coef / len(idx)
                    self.coef_[i][idx] = mean
        return self


def calculate_runtime(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()  # è®°å½•å‡½æ•°å¼€å§‹æ‰§è¡Œçš„æ—¶é—´
        result = func(*args, **kwargs)  # æ‰§è¡Œå‡½æ•°
        end_time = time.time()  # è®°å½•å‡½æ•°ç»“æŸæ‰§è¡Œçš„æ—¶é—´
        elapsed_time = end_time - start_time  # è®¡ç®—å‡½æ•°è¿è¡Œæ—¶é—´
        logging.info(f"å‡½æ•° {func.__name__} è¿è¡Œæ—¶é—´: {elapsed_time:.6f} ç§’")
        return result

    return wrapper


# åˆ›å»ºå­˜å‚¨æ–‡ä»¶å¤¹çš„å‡½æ•°
def create_folder(folder_name):
    folder_path = os.path.join(os.getcwd(), folder_name)
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    return folder_path


# ç®±å‹å›¾è¿‡æ»¤å‡½æ•°
def box_filters(df):
    for col in df.columns:
        # æ—¶é—´åˆ—å’Œè®¾å¤‡åˆ—ä¸è¿‡æ»¤
        # exclusive_column=df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)
        date_col = df.filter(like='æ—¶é—´').columns[0]
        device_col = df.filter(like='è®¾å¤‡').columns[0]
        if col not in [date_col, device_col]:
            Q1 = df.quantile(0.25)
            Q3 = df.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
    return df[(df >= lower_bound) & (df <= upper_bound)].dropna()


# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¿›è¡Œç®±å‹å›¾è¿‡æ»¤
def box_filter(df, columns):
    filtered_df = df.copy()
    for col in columns:
        Q1 = filtered_df[col].quantile(0.25)
        Q3 = filtered_df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        filtered_df = filtered_df[(filtered_df[col] >= lower_bound) & (filtered_df[col] <= upper_bound)]
    return filtered_df.dropna()


# 3-sigmaè¿‡æ»¤å‡½æ•°
def sigma_filter(df, columns):
    filtered_df = df.copy()
    for col in columns:
        # æ—¶é—´åˆ—å’Œè®¾å¤‡åˆ—ä¸è¿‡æ»¤
        mean = filtered_df[col].mean()
        std_dev = filtered_df[col].std()
        lower_bound = mean - 3 * std_dev
        upper_bound = mean + 3 * std_dev
        filtered_df = filtered_df[(filtered_df[col] >= lower_bound) & (filtered_df[col] <= upper_bound)]
    return filtered_df.dropna()


# ç§»åŠ¨å¹³å‡è¿‡æ»¤å‡½æ•°
def moving_average_filter(df, columns, window=5):
    filtered_df = df.copy()
    for col in columns:
        filtered_df[col + f"move_average"] = filtered_df[col].rolling(window=window).mean()
    return filtered_df.dropna()


# é€‰æ‹©è¿‡æ»¤æ–¹å¼
def filter_data(df, method):
    # è¿›è¡Œç®±å‹å›¾è¿‡æ»¤
    date_col = df.filter(like='æ—¶é—´').columns[0]
    device_col = df.filter(like='è®¾å¤‡').columns[0]
    inclusive_column = df.iloc[:, ~df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)].columns
    if method == 'ç®±å‹å›¾è¿‡æ»¤':
        return box_filter(df, inclusive_column)
    elif method == '3-Sigmaè¿‡æ»¤':
        return sigma_filter(df, inclusive_column)
    elif method == 'ç§»åŠ¨å¹³å‡è¿‡æ»¤':
        return moving_average_filter(df, inclusive_column)
    else:
        return df


def create_sidebar():
    st.sidebar.title("ç‰¹å¾å·¥ç¨‹6å¤§æ­¥éª¤")
    # ä¾§è¾¹æ 
    with st.sidebar:
        # æ£€æŸ¥session_stateä¸­æ˜¯å¦å·²åˆå§‹åŒ–æ­¥éª¤çŠ¶æ€ï¼Œè‹¥æ²¡æœ‰åˆ™è¿›è¡Œåˆå§‹åŒ–
        if 'step_states' not in st.session_state:
            st.session_state.step_states = {f'step{i}': False for i in range(1, 7)}

        if st.button("1ã€ä¸Šä¼ æ•°æ®", key="step1_button"):
            st.session_state.step_states['step1'] = not st.session_state.step_states['step1']

        if st.button("2ã€æ•°æ®å¯è§†åŒ–", key="step2_button"):
            st.session_state.step_states['step2'] = not st.session_state.step_states['step2']

        if st.session_state.step_states['step2']:
            st.markdown("- 2.1ã€è¶‹åŠ¿å›¾")
            st.markdown("- 2.2ã€æ•£ç‚¹å›¾")
            st.markdown("- 2.3ã€ç›´æ–¹å›¾")
            st.markdown("- 2.4ã€ç®±å‹å›¾")

        if st.button("3ã€æ•°æ®æ¸…æ´—", key="step3_button"):
            st.session_state.step_states['step3'] = not st.session_state.step_states['step3']

        if st.session_state.step_states['step3']:
            st.markdown("- 3.1ã€ç®±å‹å›¾è¿‡æ»¤")
            st.markdown("- 3.2ã€3-sigmaè¿‡æ»¤")
            st.markdown("- 3.3ã€ç§»åŠ¨å¹³å‡è¿‡æ»¤")

        if st.button("4ã€å·¥å†µåˆ†å‰²", key="step4_button"):
            st.session_state.step_states['step4'] = not st.session_state.step_states['step4']

        if st.session_state.step_states['step4']:
            st.markdown("- 4.1ã€å±‚æ¬¡èšç±»")
            st.markdown("- 4.2ï¼šKmeansèšç±»")

        if st.button("5ã€ç‰¹å¾ç”Ÿæˆ", key="step5_button"):
            st.session_state.step_states['step5'] = not st.session_state.step_states['step5']

        if st.session_state.step_states['step5']:
            st.markdown("- 5.1ã€tsfreshç‰¹å¾ç”Ÿæˆ")
            st.markdown("- 5.2ï¼šECSM(Exceedance Combination Selection Model)ç‰¹å¾ç”Ÿæˆ")

        # æ­¥éª¤1
        if st.button("6ã€ç‰¹å¾é€‰æ‹©", key="step6_button"):
            st.session_state.step_states['step6'] = not st.session_state.step_states['step6']

        if st.session_state.step_states['step6']:
            st.markdown("- 6.1ï¼šfilter-æ–¹å·®é€‰æ‹©æ³•")
            st.markdown("- 6.2ï¼šfilter-å¡æ–¹æ£€éªŒ")
            st.markdown("- 6.3ï¼šfilter-ç›¸å…³ç³»æ•°æ³•")
            st.markdown("- 6.4ï¼šwrapper-RFEé€’å½’ç‰¹å¾æ¶ˆé™¤æ³•")
            st.markdown("- 6.5ï¼šembedded-åŸºäºL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•")
            st.markdown("- 6.6ï¼šembedded-ç»“åˆL1å’ŒL2æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•")
            st.markdown("- 6.7ï¼šembedded-åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•")
            st.markdown("- 6.8ï¼šç»“åˆSVMå’ŒL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©")
            st.markdown("- 6.9ï¼šLASSO")


def feature_selection_variance(df_generated_copy, df_noNA, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©-æ–¹å·®é€‰æ‹©æ³•
    :return:
    """
    ## ç‰¹å¾é€‰æ‹©filter-æ–¹å·®é€‰æ‹©æ³•,å‚æ•°thresholdä¸ºæ–¹å·®çš„é˜ˆå€¼
    std_selector = VarianceThreshold(threshold=0.1)
    std_select_array = std_selector.fit_transform(df_noNA)
    # print(selector.variances_) #æ¯ä¸ªç‰¹å¾å€¼çš„æ ‡å‡†å·®
    # print(selector.get_support(indices=True)) #é€‰æ‹©çš„ç‰¹å¾å€¼çš„index

    # ç‰¹å¾é€‰æ‹©filter-æ–¹å·®é€‰æ‹©æ³•ï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_filter_std = pd.DataFrame(std_select_array,
                                         columns=df_generated_copy.columns[std_selector.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©filter-æ–¹å·®é€‰æ‹©æ³•ï¼Œè®¡ç®—é€‰æ‹©åçš„ç‰¹å¾çš„æ ‡å‡†å·®
    output_X_selected_std = np.std(X_selected_filter_std, axis=0)
    output_std_df = output_X_selected_std.to_frame()
    output_std_df['ç‰¹å¾åç§°'] = output_std_df.index
    output_std_df.columns = ['æ ‡å‡†å·®', 'ç‰¹å¾åç§°']

    # ç‰¹å¾é€‰æ‹©filter-æ–¹å·®é€‰æ‹©æ³•ï¼Œé€‰æ‹©çš„ç‰¹å¾å’Œè¡¡é‡ç‰¹å¾çš„æŒ‡æ ‡ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_filter_std.to_excel(os.path.join(result_folder, subfolders['filter_variance_selection_folder'],
                                                'é€‰æ‹©çš„ç‰¹å¾-filter-æ–¹å·®é€‰æ‹©æ³•.xlsx'))
    output_std_df.to_excel(os.path.join(result_folder, subfolders['filter_variance_selection_folder'],
                                        'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-filter-æ–¹å·®é€‰æ‹©æ³•.xlsx'))
    st.success("æ–¹å·®é€‰æ‹©æ³•å·²å®Œæˆ!")


def feature_selection_chi2_test(df_generated_copy, df_MinMax, y, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©-å¡æ–¹æ£€éªŒ
    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œé€‚ç”¨äºç¦»æ•£å˜é‡ï¼Œè¦æ±‚æ•°å€¼ä¸ºæ­£å€¼ï¼Œä½¿ç”¨åŒºé—´ç¼©æ”¾åä¸”å¡«å……è¿‡ç¼ºå¤±å€¼çš„æ•°æ®
    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œé€‰æ‹©Kä¸ªæœ€å¥½çš„ç‰¹å¾ï¼Œè¿”å›é€‰æ‹©ç‰¹å¾åçš„æ•°æ®
    :return:
    """
    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œåˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(df_MinMax, y, test_size=0.2, random_state=0)

    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œä½¿ç”¨å¡æ–¹é€‰æ‹©ï¼Œé€‰æ‹©å•å˜é‡ç‰¹å¾é€‰æ‹©Kä¸ªæœ€ä½³ç‰¹å¾
    chi2(X_train, y_train)  # æŸ¥çœ‹å¡æ–¹çš„ä¸¤ä¸ªæŒ‡æ ‡ï¼Œä¸€æ˜¯å„ä¸ªç‰¹å¾å˜é‡çš„å¡æ–¹ç»Ÿè®¡é‡å€¼ï¼ŒäºŒæ˜¯å„ä¸ªç‰¹å¾å˜é‡çš„Pç»Ÿè®¡é‡å€¼ï¼Œ
    k = 10  # è¦é€‰æ‹©çš„ç‰¹å¾æ•°
    chi2_selector = SelectKBest(chi2, k=k)
    chi2_selector_array = chi2_selector.fit_transform(X_train, y_train)

    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_filter_chi = pd.DataFrame(chi2_selector_array,
                                         columns=df_generated_copy.columns[chi2_selector.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œè¾“å‡ºé€‰æ‹©çš„ç‰¹å¾ã€å¯¹åº”çš„å¡æ–¹å€¼å’ŒP-value
    output_chi_df = pd.DataFrame({'Feature': df_generated_copy.columns[chi2_selector.get_support(indices=True)],
                                  'Chi-Square': chi2_selector.scores_[chi2_selector.get_support(indices=True)],
                                  'P-Value': chi2_selector.pvalues_[chi2_selector.get_support(indices=True)]})

    # ç‰¹å¾é€‰æ‹©filter-å¡æ–¹æ£€éªŒï¼Œé€‰æ‹©çš„ç‰¹å¾å’Œè¡¡é‡ç‰¹å¾çš„æŒ‡æ ‡ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_filter_chi.to_excel(
        os.path.join(result_folder, subfolders['filter_chi2_estimator_folder'], 'é€‰æ‹©çš„ç‰¹å¾-filter-å¡æ–¹æ£€éªŒ.xlsx'))
    output_chi_df.to_excel(os.path.join(result_folder, subfolders['filter_chi2_estimator_folder'],
                                        'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-filter-å¡æ–¹æ£€éªŒ.xlsx'))
    st.success("å¡æ–¹æ£€éªŒæ³•å·²å®Œæˆ!")


def feature_selection_correlation_coefficient(df_generated_copy, y, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•
    ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œåˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨ç›¸å…³ç³»æ•°å’Œpå€¼
    :return:
    """
    correlation_dict = {}
    features = df_generated_copy.columns

    # ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œè®¡ç®—ç›¸å…³ç³»æ•°
    for feature in df_generated_copy.columns:
        corr, p_value = pearsonr(df_generated_copy[feature], y)
        correlation_dict[feature] = (corr, p_value)

    # ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œå°†ç›¸å…³ç³»æ•°è½¬æ¢ä¸ºDataFrameä»¥ä¾¿æŸ¥çœ‹
    correlation_df = pd.DataFrame.from_dict(correlation_dict, orient='index', columns=['Correlation', 'P-value'])

    # ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œè®¾ç½®ç›¸å…³ç³»æ•°é€‰æ‹©çš„é˜ˆå€¼
    threshold = 0.5

    # ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œé€‰æ‹©ç›¸å…³æ€§å¼ºçš„ç‰¹å¾ï¼Œè¾“å‡ºé€‰æ‹©çš„ç‰¹å¾çš„ç›¸å…³ç³»æ•°å’Œç‰¹å¾è¡¨
    output_correlation_df = correlation_df[correlation_df['Correlation'].abs() > threshold]
    selected_features = correlation_df[correlation_df['Correlation'].abs() > threshold].index.tolist()
    X_selected_filter_correlation = df_generated_copy[selected_features]

    # ç‰¹å¾é€‰æ‹©filter-ç›¸å…³ç³»æ•°æ³•ï¼Œé€‰æ‹©çš„ç‰¹å¾å’Œè¡¡é‡ç‰¹å¾çš„æŒ‡æ ‡ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_filter_correlation.to_excel(
        os.path.join(result_folder, subfolders['filter_correlation_coefficient_folder'],
                     'é€‰æ‹©çš„ç‰¹å¾-filter-ç›¸å…³ç³»æ•°æ³•.xlsx'))
    output_correlation_df.to_excel(os.path.join(result_folder, subfolders['filter_correlation_coefficient_folder'],
                                                'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-filter-ç›¸å…³ç³»æ•°æ³•.xlsx'))
    st.success("ç›¸å…³ç³»æ•°æ³•å·²å®Œæˆ!")


def feature_selection_RFE(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©wrapper-RFEé€’å½’ç‰¹å¾æ¶ˆé™¤æ³•
    :return:
    """
    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œåˆ›å»ºä¸€ä¸ªç”¨äºRFEé€‰æ‹©ç‰¹å¾çš„åˆ†ç±»æ¨¡å‹
    rfe_selector = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)

    ## ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œæ¨¡å‹æ‹ŸåˆRFE
    rfe_selector.fit(df_noNA, y)
    rfe_selector_array = rfe_selector.fit_transform(df_noNA, y)

    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_wrapper_rfe = pd.DataFrame(rfe_selector_array,
                                          columns=df_generated_copy.columns[rfe_selector.get_support(indices=True)])

    ## ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œä¿å­˜é€‰æ‹©çš„ç‰¹å¾å’Œranking
    record_result_all = []
    # print('Selected Features:')
    for i in range(len(df_generated_copy.columns)):
        output_wrapper_rfe = pd.DataFrame()
        if rfe_selector.support_[i]:
            #        print(df_copy.columns[i])
            output_wrapper_rfe['feature'] = [df_generated_copy.columns[i]]
            output_wrapper_rfe['ranking'] = [rfe_selector.ranking_[i]]
            record_result_all.append(output_wrapper_rfe)
    output_wrapper_rfe_selected = pd.concat(record_result_all)

    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œè·å–å…¨éƒ¨ç‰¹å¾çš„ranking
    features = df_generated_copy.columns
    rfe_feature_ranking = rfe_selector.ranking_
    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œåˆ›å»ºå…¨éƒ¨ç‰¹å¾rankingçš„DataFrame
    output_wrapper_rfe_all = pd.DataFrame({'Feature': features, 'Ranking': rfe_feature_ranking})
    output_wrapper_rfe_all = output_wrapper_rfe_all.sort_values(by='Ranking')

    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œé€‰æ‹©çš„ç‰¹å¾å’Œè¡¡é‡ç‰¹å¾çš„æŒ‡æ ‡ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_wrapper_rfe.to_excel(
        os.path.join(result_folder, subfolders['filter_RFE_folder'], 'é€‰æ‹©çš„ç‰¹å¾-wrapper-RFE.xlsx'))
    output_wrapper_rfe_selected.to_excel(
        os.path.join(result_folder, subfolders['filter_RFE_folder'], 'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-wrapper-RFE.xlsx'))
    output_wrapper_rfe_all.to_excel(
        os.path.join(result_folder, subfolders['filter_RFE_folder'], 'å…¨éƒ¨ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-wrapper-RFE.xlsx'))

    # ç‰¹å¾é€‰æ‹©wrapper-RFEï¼Œé€‰æ‹©çš„ç‰¹å¾çš„rankingå¯è§†åŒ–
    plt.figure(figsize=(10, 15))
    sns.barplot(x='feature', y='ranking', data=output_wrapper_rfe_selected)
    plt.xticks(rotation=90)
    plt.title('Feature Ranking')
    plt.xlabel('Ranking')
    plt.ylabel('Feature')
    plt.savefig(os.path.join(result_folder, subfolders['filter_RFE_folder'], 'ç‰¹å¾å¯è§†åŒ–-wrapper-RFE.jpg'))
    st.success("RFEæ–¹æ³•å·²å®Œæˆ!")


def feature_selection_embedded_based_on_L1(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    åŸºäºL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•
    :return:
    """
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼Œå¸¦L1æƒ©ç½šé¡¹çš„é€»è¾‘å›å½’ä½œä¸ºåŸºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    penalty_selector_L1 = SelectFromModel(LogisticRegression(penalty="l1",
                                                             C=0.1,
                                                             solver='liblinear'))
    # æŠ¥é”™ï¼ŒæŒ‡å®šæ±‚è§£å™¨ï¼Œé”™è¯¯å°±ä¼šæ¶ˆå¤±ã€‚l1 æ”¯æŒ â€˜liblinearâ€™ å’Œ â€˜sagaâ€™ L2 å¤„ç† newton-cgâ€™ã€â€™lbfgsâ€™ã€â€™sagâ€™ å’Œ â€˜sagaâ€™
    penalty_selector_array_L1 = penalty_selector_L1.fit_transform(df_noNA, y)
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_embedded_penalty_L1 = pd.DataFrame(penalty_selector_array_L1,
                                                  columns=df_generated_copy.columns[
                                                      penalty_selector_L1.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼ŒL1é€‰æ‹©çš„ç‰¹å¾ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_embedded_penalty_L1.to_excel(
        os.path.join(result_folder, subfolders['filter_based_on_L1_folder'], 'é€‰æ‹©çš„ç‰¹å¾-embedded-penalty_L1.xlsx'))
    st.success("åŸºäºL1çš„embeddedç‰¹å¾é€‰æ‹©æ–¹æ³•å·²å®Œæˆ!")


def feature_selection_embedded_based_on_L1L2(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    åŸºäºL1å’ŒL2æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•
    :return:
    """
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼ŒL1æƒ©ç½šé¡¹é™ç»´çš„åŸç†åœ¨äºä¿ç•™å¤šä¸ªå¯¹ç›®æ ‡å€¼å…·æœ‰åŒç­‰ç›¸å…³æ€§çš„ç‰¹å¾ä¸­çš„ä¸€ä¸ªï¼Œæ‰€ä»¥æ²¡é€‰åˆ°çš„ç‰¹å¾ä¸ä»£è¡¨ä¸é‡è¦ï¼Œå¯ç»“åˆL2æƒ©ç½šé¡¹æ¥ä¼˜åŒ–ï¼Œæ“ä½œå¦‚ä¸‹ï¼š
    # è‹¥ä¸€ä¸ªç‰¹å¾åœ¨L1ä¸­çš„æƒå€¼ä¸º1ï¼Œé€‰æ‹©åœ¨L2ä¸­æƒå€¼å·®åˆ«ä¸å¤§ä¸”åœ¨L1ä¸­æƒå€¼ä¸º0çš„ç‰¹å¾æ„æˆåŒç±»é›†åˆï¼Œå°†è¿™ä¸€é›†åˆä¸­çš„ç‰¹å¾å¹³åˆ†L1ä¸­çš„æƒå€¼ï¼Œæ•…éœ€è¦æ„å»ºä¸€ä¸ªæ–°çš„é€»è¾‘å›å½’æ¨¡å‹ã€‚
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼Œå¸¦L1å’ŒL2æƒ©ç½šé¡¹çš„é€»è¾‘å›å½’ä½œä¸ºåŸºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©ï¼Œå‚æ•°thresholdä¸ºæƒå€¼ç³»æ•°ä¹‹å·®çš„é˜ˆå€¼
    penalty_selector_L1L2 = SelectFromModel(LR(threshold=0.5, C=0.1))
    penalty_selector_array_L1L2 = penalty_selector_L1L2.fit_transform(df_noNA, y)
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_embedded_penalty_L1L2 = pd.DataFrame(penalty_selector_array_L1L2, columns=df_generated_copy.columns[
        penalty_selector_L1L2.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼ŒL1L2é€‰æ‹©çš„ç‰¹å¾ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_embedded_penalty_L1L2.to_excel(os.path.join(result_folder, subfolders['filter_based_on_L1L2_folder'],
                                                           'é€‰æ‹©çš„ç‰¹å¾-embedded-penalty_L1å’ŒL2.xlsx'))
    st.success("åŸºäºL1å’ŒL2çš„embeddedç‰¹å¾é€‰æ‹©æ–¹æ³•å·²å®Œæˆ!")


def feature_selection_embedded_based_on_SVM_L1(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    ä½¿ç”¨ feature_selection åº“çš„ SelectFromModel ç±»ç»“åˆ SVM æ¨¡å‹
    :return:
    """
    lsvc_selector = LinearSVC(C=0.01, penalty='l1', dual=False).fit(df_noNA, y)
    lsvc_model = SelectFromModel(lsvc_selector, prefit=True)
    X_sfm_svm_array = lsvc_model.transform(df_noNA)
    # å°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_svm = pd.DataFrame(X_sfm_svm_array,
                                  columns=df_generated_copy.columns[lsvc_model.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©SVMå’Œpenalty_L1ï¼Œé€‰æ‹©çš„ç‰¹å¾ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_svm.to_excel(
        os.path.join(result_folder, subfolders['filter_based_on_SVM_L1_folder'], 'é€‰æ‹©çš„ç‰¹å¾-SVMå’Œpenalty_L1.xlsx'))
    st.success("åŸºäºL1å’ŒSVMçš„embeddedç‰¹å¾é€‰æ‹©æ–¹æ³•å·²å®Œæˆ!")


def feature_selection_embedded_based_on_GBDT(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©embedded-åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•,æ ‘æ¨¡å‹ä¸­GBDTä¹Ÿå¯ç”¨æ¥ä½œä¸ºåŸºæ¨¡å‹è¿›è¡Œç‰¹å¾é€‰æ‹©
    :return:
    """
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼ŒGBDTä½œä¸ºåŸºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    gbdt_selector = SelectFromModel(GradientBoostingClassifier())
    gbdt_selector_array = gbdt_selector.fit_transform(df_noNA, y)
    # ç‰¹å¾é€‰æ‹©embedded-åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•ï¼Œå°†é€‰æ‹©åçš„ç‰¹å¾è½¬æ¢ä¸ºDataFrame
    X_selected_embedded_gbdt = pd.DataFrame(gbdt_selector_array,
                                            columns=df_generated_copy.columns[gbdt_selector.get_support(indices=True)])

    # ç‰¹å¾é€‰æ‹©embedded-GBDTï¼Œé€‰æ‹©çš„ç‰¹å¾ä¿å­˜ä¸ºæ–‡ä»¶
    X_selected_embedded_gbdt.to_excel(
        os.path.join(result_folder, subfolders['filter_base_on_GBDT_folder'], 'é€‰æ‹©çš„ç‰¹å¾-embedded-GBDT.xlsx'))
    st.success("åŸºäºGBDTçš„embeddedç‰¹å¾é€‰æ‹©æ–¹æ³•å·²å®Œæˆ!")


def feature_selection_embedded_based_on_Lasso(df_generated_copy, y, result_folder, subfolders):
    """
    ç‰¹å¾é€‰æ‹©embedded-åŸºäºLassoçš„ç‰¹å¾é€‰æ‹©æ³•
    :return:
    """
    # åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, Y_train, Y_test = train_test_split(df_generated_copy, y, random_state=100)

    # æ„é€ ä¸åŒçš„lambdaå€¼
    Lambdas = np.logspace(-5, 2, 200)

    # è®¾ç½®äº¤å‰éªŒè¯çš„å‚æ•°ï¼Œä½¿ç”¨å‡æ–¹è¯¯å·®è¯„ä¼°
    lasso_cv = LassoCV(alphas=Lambdas, cv=10, max_iter=10000)
    lasso_cv.fit(X_train, Y_train)

    # åŸºäºæœ€ä½³lambdaå€¼å»ºæ¨¡
    lasso_selector = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)  # åˆ é™¤normalize

    lasso_selector.fit(X_train, Y_train)

    # æ¨¡å‹è¯„ä¼°
    lasso_pred = lasso_selector.predict(X_test)

    # å‡æ–¹è¯¯å·®
    MSE = mean_squared_error(Y_test, lasso_pred)

    # è¾“å‡ºæ¯ä¸ªç‰¹å¾çš„ç³»æ•°
    output_lasso_coef = pd.DataFrame({'Feature': X_train.columns, 'coefficient': lasso_selector.coef_.tolist()})
    output_lasso_coef.to_excel(
        os.path.join(result_folder, subfolders['filter_based_on_Lasso_folder'], 'å…¨éƒ¨ç‰¹å¾çš„ç³»æ•°-lasso.xlsx'))

    # å›ºå®šalphaï¼Œè®­ç»ƒLassoæ¨¡å‹ï¼Œå±•ç¤ºæ¯ä¸ªç‰¹å¾çš„é¢„æµ‹R2
    alpha = 0.1
    lasso_fix_alpha = Lasso(alpha=alpha)
    y_pred_lasso_fix_alpha = lasso_fix_alpha.fit(X_train, Y_train).predict(X_test)
    r2_score_lasso_fix_alpha = r2_score(Y_test, y_pred_lasso_fix_alpha)

    plt.plot(lasso_fix_alpha.coef_, color='gold', linewidth=2, label='Lasso coefficients')
    plt.title(f"Lasso R^2: {r2_score_lasso_fix_alpha}")
    plt.savefig(os.path.join(result_folder, subfolders['filter_based_on_Lasso_folder'], 'lasso_coefficients.png'))
    st.success("åŸºäºLASSOçš„embeddedç‰¹å¾é€‰æ‹©æ–¹æ³•å·²å®Œæˆ!")


def feature_importance_estimate(df_generated_copy, df_noNA, y, result_folder, subfolders):
    """
    ç‰¹å¾é‡è¦æ€§è¯„ä¼°
    ç‰¹å¾é€‰æ‹©ï¼Œæ’åˆ—é‡è¦æ€§è¯„ä¼°ï¼Œå±•ç¤ºé‡è¦æ€§å˜åŒ–
    :return:
    """
    # åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, Y_train, Y_test = train_test_split(df_noNA, y, random_state=100)

    # ç”¨éšæœºæ£®æ—åšæ¨¡å‹
    Permutation_selector = RandomForestClassifier(n_estimators=100,
                                                  bootstrap=True,
                                                  max_features='sqrt')  # bootstrap

    # åœ¨è®­ç»ƒé›†æ‹Ÿåˆæ¨¡å‹
    Permutation_selector_array = Permutation_selector.fit(X_train, Y_train)

    # ä½œå›¾å±•ç¤ºé‡è¦æ€§
    # perm = PermutationImportance(Permutation_selector, random_state=10).fit(X_test, Y_test)
    # html_content = eli5.show_weights(perm, feature_names=df_copy.columns.tolist())
    # ç”Ÿæˆçš„htmlæ–‡ä»¶ï¼Œå¯åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ã€‚(æˆåŠŸ)
    # ç”Ÿæˆçš„htmlæ–‡ä»¶è½¬æˆimageçš„æ–¹å¼ã€‚ï¼ˆå¤±è´¥ï¼‰
    # with open(os.path.join(result_folder,subfolders['feature_importance_folder'],'feature_importance.html'), 'w', encoding='gbk') as f:
    #     f.write(html_content.data)

    # ç‰¹å¾é€‰æ‹©ï¼Œæ’åˆ—é‡è¦æ€§è¯„ä¼°ï¼Œå±•ç¤ºé‡è¦æ€§ä¸‹é™
    # åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, Y_train, Y_test = train_test_split(df_noNA, y, random_state=100)

    # ç”¨éšæœºæ£®æ—åšæ¨¡å‹
    perm_RandomForest_selector = RandomForestClassifier(n_estimators=100, random_state=1)

    # åœ¨è®­ç»ƒé›†æ‹Ÿåˆæ¨¡å‹
    perm_RandomForest_selector.fit(X_train, Y_train)

    # åœ¨æµ‹è¯•é›†è®¡ç®—å¾—åˆ†ä½œä¸ºbaseline
    permutation_baseline = perm_RandomForest_selector.score(X_test, Y_test)

    # åœ¨æµ‹è¯•é›†æ”¹å˜ç‰¹å¾çš„é¡ºåº10æ¬¡ï¼Œè®¡ç®—æ¯æ¬¡çš„å¾—åˆ†ã€å¾—åˆ†çš„å‡å€¼å’Œæ ‡å‡†å·®
    permutation_result = permutation_importance(perm_RandomForest_selector,
                                                X_test,
                                                Y_test,
                                                n_repeats=10,
                                                random_state=1,
                                                scoring='accuracy')
    importances = permutation_result.importances_mean

    # è¾“å‡ºæ¯ä¸ªç‰¹å¾å¤šæ¬¡æ”¹å˜é¡ºåºçš„å¾—åˆ†çš„å¹³å‡å€¼
    output_permutation_importances = pd.DataFrame(
        {'Feature': df_generated_copy.columns, 'importances_mean': importances})

    # ç‰¹å¾é€‰æ‹©ï¼Œæ’åˆ—é‡è¦æ€§è¯„ä¼°ï¼Œæ¯ä¸ªç‰¹å¾çš„å¾—åˆ†çš„å¹³å‡å€¼ä¿å­˜ä¸ºæ–‡ä»¶
    output_permutation_importances.to_excel(os.path.join(result_folder, subfolders['feature_importance_folder'],
                                                         'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-æ’åˆ—é‡è¦æ€§è¯„ä¼°-å¤šæ¬¡æ’åºçš„å¾—åˆ†å¹³å‡å€¼.xlsx'))

    # ç”»å›¾ï¼Œè¿è¡Œç»“æœï¼Œé‡è¦æ€§ä¸‹é™çš„è¶Šå¤šï¼Œè¯´æ˜è¯¥ç‰¹å¾è¶Šé‡è¦
    plt.figure(figsize=(20, 8))
    sns.barplot(x='importances_mean', y='Feature', data=output_permutation_importances)
    plt.title('Feature importances_mean', fontsize=20)
    plt.xlabel('importances_mean', fontsize=20)
    plt.ylabel('Feature', fontsize=3)
    plt.savefig(
        os.path.join(result_folder, subfolders['feature_importance_folder'], 'æ’åˆ—é‡è¦æ€§è¯„ä¼°-å¤šæ¬¡æ’åºçš„å¾—åˆ†å¹³å‡å€¼.png'))

    # ç‰¹å¾é€‰æ‹©ï¼Œé‡è¦æ€§è¯„ä¼°
    # ç”¨æåº¦éšæœºæ ‘åšæ¨¡å‹ï¼Œè¯„ä¼°å‚æ•°çš„é‡è¦æ€§
    importances_selector = ExtraTreesClassifier()
    importances_selector.fit(df_noNA, y)

    X_importances = list(zip(df_generated_copy.columns, importances_selector.feature_importances_))
    output_importances = pd.DataFrame(X_importances, columns=['feature', 'importances'])
    output_importances = output_importances.sort_values(by='importances', ascending=False).head(20)

    # ç‰¹å¾é€‰æ‹©ï¼Œæ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ä¿å­˜ä¸ºæ–‡ä»¶
    output_importances.to_excel(
        os.path.join(result_folder, subfolders['feature_importance_folder'], 'é€‰æ‹©çš„ç‰¹å¾çš„è¡¡é‡æŒ‡æ ‡-é‡è¦æ€§è¯„ä¼°.xlsx'))


def data_read_and_process(second_uploaded_file):
    """
    è¯»å–æ•°æ®å¹¶å¤„ç†
    :return:
    """
    if second_uploaded_file:
        # è¯»å…¥æŒ¯åŠ¨ä¿¡å·çš„FFTç‰¹å¾å‚æ•°ï¼Œæˆ–FFTç‰¹å¾å‚æ•°çš„tsfreshç”Ÿæˆçš„ç‰¹å¾
        df_generated = pd.read_excel(second_uploaded_file)
        df_generated_copy = df_generated.copy()

        y = df_generated['æ•…éšœæ ‡è®°']  # ä»åŸå§‹æ•°æ®è·å–æ•…éšœåˆ—ï¼Œå› å˜é‡y

        columns = df_generated_copy.columns  # åˆ é™¤è‡ªå˜é‡Xä¸éœ€è¦çš„åˆ—
        if 'è®¾å¤‡ç¼–ç ' in columns:
            del df_generated_copy['è®¾å¤‡ç¼–ç ']
        if 'æ•…éšœæ ‡è®°' in columns:
            del df_generated_copy['æ•…éšœæ ‡è®°']
        if 'ä¿¡å·æ—¶é—´' in columns:
            del df_generated_copy['ä¿¡å·æ—¶é—´']
        if 'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´' in columns:
            del df_generated_copy['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´']
        if 'æ•…éšœæ—¶é—´åˆ†ç»„' in columns:
            del df_generated_copy['æ•…éšœæ—¶é—´åˆ†ç»„']
        return df_generated, df_generated_copy, y


def data_preprocessing(df_copy):
    """
    å¤„ç†æ•°æ®
    1ã€æ ‡å‡†åŒ–ã€
    2ã€å½’ä¸€åŒ–ã€
    3ã€åŒºé—´ç¼©æ”¾ã€
    4ã€äºŒå€¼åŒ–
    5ã€ç¼ºå¤±å€¼å¤„ç†

    :return:
    """
    # æ•°æ®é¢„å¤„ç†
    df_standard = StandardScaler().fit_transform(df_copy)

    # åŒºé—´ç¼©æ”¾ï¼Œè¿”å›å€¼ä¸º[0,1]åŒºé—´çš„æ•°æ®
    df_MinMax = MinMaxScaler().fit_transform(df_copy)

    # å½’ä¸€åŒ–ï¼Œè¿”å›å€¼ä¸ºå½’ä¸€åŒ–åçš„æ•°æ®
    df_normalize = Normalizer().fit_transform(df_copy)

    # å¯¹æ•°æ®è¿›è¡ŒäºŒå€¼åŒ–,äºŒå€¼åŒ–ï¼Œè®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼Œä¾‹å¦‚3ï¼Œå¤§äºé˜ˆå€¼çš„èµ‹å€¼ä¸º1ï¼Œåä¹‹ä¸º0ï¼Œè¿”å›å€¼ä¸ºäºŒå€¼åŒ–åçš„æ•°æ®
    df_binary = Binarizer(threshold=3).fit_transform(df_copy)

    # å‚æ•°missing_valueä¸ºç¼ºå¤±å€¼çš„è¡¨ç¤ºå½¢å¼ï¼Œé»˜è®¤ä¸ºNaN,å‚æ•°strategyä¸ºç¼ºå¤±å€¼å¡«å……æ–¹å¼ï¼Œé»˜è®¤ä¸ºmean
    null_numbers = df_copy.isnull().sum().sum()
    if null_numbers == 0:
        df_noNA = df_copy
    else:
        df_noNA = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(df_copy)

    return df_noNA, df_standard, df_MinMax, df_normalize, df_binary


def step_one_data_uploader():
    # ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ æ•°æ®
    st.subheader('æ­¥éª¤1:ä¸Šä¼ æ•°æ®')
    st.warning('æç¤ºï¼šéœ€è¦åŒæ—¶ä¸Šä¼ ä¸¤ç±»æ•°æ®ï¼š'
               '1ã€æ ¼å¼åŒ–ç‰¹å¾é›†æ•°æ®'
               '2ã€äººå·¥æ ‡è®°æ•…éšœæ—¶æ®µæ•°æ®ã€‚\næ”¯æŒä¸Šä¼ å¤šä¸ªæ ¼å¼åŒ–ç‰¹å¾é›†æ•°æ®ã€‚')
    st.info("â„¹ï¸ è¯·ä¸Šä¼ æ–‡ä»¶ä»¥è¿›è¡Œæ£€æŸ¥ã€‚")
    uploaded_files = st.file_uploader("ä¸Šä¼ ä½ çš„æ•°æ®æ–‡ä»¶", type=["csv", "xlsx", "xls"], key='upload_original_data',
                                      accept_multiple_files=True)

    # æ£€æŸ¥ä¸Šä¼ æ–‡ä»¶çš„æ ¼å¼
    if uploaded_files:
        # è·å–ä¸Šä¼ æ–‡ä»¶çš„æ‰©å±•å
        file_extensions = [file.name.lower() for file in uploaded_files]

        # æ£€æŸ¥æ˜¯å¦åŒ…å« CSV å’Œ Excel æ–‡ä»¶
        has_csv = any("æ ¼å¼åŒ–ç‰¹å¾é›†.csv" in ext for ext in file_extensions)
        has_excel = any('äººå·¥æ ‡è®°æ•…éšœæ—¶æ®µ' in ext for ext in file_extensions)

        # åˆ¤æ–­æ˜¯å¦åŒæ—¶åŒ…å« CSV å’Œ Excel
        if not (has_csv and has_excel):
            st.error("âŒ ä¸Šä¼ æ–‡ä»¶ä¸å¯¹ï¼è¯·ç¡®ä¿ä¸Šä¼ çš„æ•°æ®åŒæ—¶åŒ…å«'æ ¼å¼åŒ–ç‰¹å¾é›†'å’Œ'äººå·¥æ ‡è®°æ•…éšœæ—¶æ®µ'")
        else:
            st.success("âœ… æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼")

    read_data_list = []
    data = None

    for uploaded_file in uploaded_files:
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶åŠ è½½åç¼€åä¸º_æ ¼å¼åŒ–ç‰¹å¾é›†.csvçš„æ•°æ®
        if uploaded_file.name.endswith('_æ ¼å¼åŒ–ç‰¹å¾é›†.csv'):
            df = pd.read_csv(uploaded_file)
            drop_col = df.filter(like='Unnamed: 0', axis='columns')
            if not drop_col.empty:
                df.drop(columns=drop_col, inplace=True)
            device_name = uploaded_file.name.split('.')[0][:-7]
            read_data_list.append((device_name, df))
        elif uploaded_file.name.startswith('äººå·¥æ ‡è®°æ•…éšœæ—¶æ®µ'):
            data = pd.read_excel(uploaded_file)

    return read_data_list, data


def step_two_data_visualization(read_data_list, result_folder, subfolders):
    # ç¬¬äºŒæ­¥ï¼šæ•°æ®åˆ†æå’Œå¯è§†åŒ–
    st.subheader('æ­¥éª¤2:æ•°æ®å¯è§†åŒ–')
    st.warning(
        'æç¤ºï¼šå¦‚æœä¸Šä¼ å¤šä¸ªæ–‡ä»¶ï¼Œåªä¼šé€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶å±•ç¤ºä¸€ç§ç‰¹å¾çš„è¶‹åŠ¿å›¾ã€æ•£ç‚¹å›¾ã€ç›´æ–¹å›¾å’Œç®±å‹å›¾ã€‚å…¶å®ƒæ–‡ä»¶å’Œç‰¹å¾çš„å›¾å½¢æœªå±•ç¤ºï¼Œä½†å·²ä¿å­˜ï¼')

    col1, col2, col3, col4 = st.columns(4)
    # ç”Ÿæˆè¶‹åŠ¿å›¾ã€æ•£ç‚¹å›¾ã€ç›´æ–¹å›¾ã€ç®±å‹å›¾
    with col1:
        if st.checkbox('è¶‹åŠ¿å›¾', key='è¶‹åŠ¿å›¾'):
            st.write('è¶‹åŠ¿å›¾ç¤ºä¾‹')
            for data_index, data_item in enumerate(read_data_list):
                # æ•°å€¼åˆ—
                device_name, df = data_item
                date_col = df.filter(like='æ—¶é—´').columns[0]
                device_col = df.filter(like='è®¾å¤‡').columns[0]
                df_plot = df.loc[:, ~df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)]
                df_plot.columns = df_plot.columns.str.replace('/', '_')
                if len(df_plot.columns) >= 1:
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for index, col in enumerate(df_plot.columns):
                            fig_trend, ax_trend = plt.subplots()
                            ax_trend.plot(pd.to_datetime(df[date_col]), df_plot[col])
                            ax_trend.set_title(f'({col})çš„è¶‹åŠ¿å›¾')
                            ax_trend.set_xlabel('æ—¥æœŸ')
                            ax_trend.set_ylabel(f'{col}')
                            ax_trend.set_xticklabels(ax_trend.get_xticklabels(), rotation=30)
                            trend_plot_path = os.path.join(result_folder, 'å¯è§†åŒ–ç»“æœ', f'{device_name}_å¯è§†åŒ–ç»“æœ',
                                                           subfolders['trend_dir'])
                            img_buffer = BytesIO()
                            fig_trend.savefig(img_buffer, format='png', dpi=300)
                            img_buffer.seek(0)
                            # å°†å›¾ç‰‡å†™å…¥zipæ–‡ä»¶
                            zip_file.writestr(f'{col}.png', img_buffer.getvalue())
                            if data_index == 0 and index == 0:
                                st.pyplot(fig_trend)
                            plt.close(fig_trend)
                    # æä¾› ZIP æ–‡ä»¶ä¸‹è½½
                    zip_buffer.seek(0)
                    st.download_button(
                        label=f"ä¸‹è½½{device_name}è¶‹åŠ¿å›¾",
                        data=zip_buffer,
                        file_name=f"{device_name}_è¶‹åŠ¿å›¾.zip",
                        mime="application/zip"
                    )
    with col2:
        if st.checkbox('æ•£ç‚¹å›¾', key='scatter'):
            st.write('æ•£ç‚¹å›¾ç¤ºä¾‹')
            for data_index, data_item in enumerate(read_data_list):
                device_name, df = data_item
                date_col = df.filter(like='æ—¶é—´').columns[0]
                device_col = df.filter(like='è®¾å¤‡').columns[0]
                df_plot = df.loc[:, ~df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)]
                df_plot.columns = df_plot.columns.str.replace('/', '_')
                if len(df_plot.columns) >= 1:
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for index, col in enumerate(df_plot.columns):
                            fig_scatter, ax_scatter = plt.subplots()
                            ax_scatter.scatter(pd.to_datetime(df[date_col]), df_plot[col])
                            ax_scatter.set_title(f'{col}çš„æ•£ç‚¹å›¾')
                            ax_scatter.set_xlabel('æ—¥æœŸ')
                            ax_scatter.set_ylabel(f'{col}')
                            img_buffer = BytesIO()
                            fig_scatter.savefig(img_buffer, format='png', dpi=300)
                            img_buffer.seek(0)
                            # å°†å›¾ç‰‡å†™å…¥zipæ–‡ä»¶
                            zip_file.writestr(f'{col}.png', img_buffer.getvalue())

                            if data_index == 0 and index == 0:
                                st.pyplot(fig_scatter)

                            plt.close()
                    zip_buffer.seek(0)
                    st.download_button(
                        label=f"ä¸‹è½½{device_name}æ•£ç‚¹å›¾",
                        data=zip_buffer,
                        file_name=f"{device_name}_æ•£ç‚¹å›¾.zip",
                        mime="application/zip"

                    )
    with col3:
        if st.checkbox('ç›´æ–¹å›¾', key='ç›´æ–¹å›¾'):
            st.write('ç›´æ–¹å›¾ç¤ºä¾‹')
            for data_index, data_item in enumerate(read_data_list):
                device_name, df = data_item
                date_col = df.filter(like='æ—¶é—´').columns[0]
                device_col = df.filter(like='è®¾å¤‡').columns[0]
                df_plot = df.loc[:, ~df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)]
                df_plot.columns = df_plot.columns.str.replace('/', '_')
                # ç”Ÿæˆç›´æ–¹å›¾
                if len(df_plot.columns) >= 1:
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for index, col in enumerate(df_plot.columns):
                            fig_hist, ax_hist = plt.subplots(figsize=(20, 20))
                            ax_hist.hist(df_plot[col], bins=20)
                            ax_hist.set_title(f'{col}çš„ç›´æ–¹å›¾')
                            ax_hist.set_xlabel(f'{col}')
                            ax_hist.set_ylabel('é¢‘ç‡')
                            ax_hist.grid(True)

                            img_buffer = BytesIO()
                            fig_hist.savefig(img_buffer, format='png', dpi=300)
                            img_buffer.seek(0)
                            # å°†å›¾ç‰‡å†™å…¥zipæ–‡ä»¶
                            zip_file.writestr(f'{col}.png', img_buffer.getvalue())
                            if data_index == 0 and index == 0:
                                st.pyplot(fig_hist)
                            plt.close(fig_hist)
                    zip_buffer.seek(0)
                    st.download_button(
                        label=f"ä¸‹è½½{device_name}ç›´æ–¹å›¾",
                        data=zip_buffer,
                        file_name=f"{device_name}_ç›´æ–¹å›¾.zip",
                        mime="application/zip"
                    )
    with col4:
        if st.checkbox('ç®±å‹å›¾', key='ç®±å‹å›¾'):
            st.write('ç®±å‹å›¾ç¤ºä¾‹')
            for data_index, data_item in enumerate(read_data_list):
                device_name, df = data_item
                date_col = df.filter(like='æ—¶é—´').columns[0]
                device_col = df.filter(like='è®¾å¤‡').columns[0]
                df_plot = df.loc[:, ~df.columns.str.contains('æ—¶é—´|è®¾å¤‡', case=False)]
                df_plot.columns = df_plot.columns.str.replace('/', '_')
                # ç”Ÿæˆç®±å‹å›¾
                if len(df_plot.columns) >= 1:
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for index, col in enumerate(df_plot.columns):
                            fig_box, ax_box = plt.subplots(figsize=(20, 20))
                            ax_box.boxplot(df_plot[col])
                            ax_box.set_title(f'{col}çš„ç®±å‹å›¾')
                            ax_box.set_xlabel(f'å˜é‡{col}')
                            ax_box.set_ylabel('å€¼')
                            ax_box.grid(True)
                            img_buffer = BytesIO()
                            fig_box.savefig(img_buffer, format='png', dpi=300)
                            img_buffer.seek(0)
                            # å°†å›¾ç‰‡å†™å…¥zipæ–‡ä»¶
                            zip_file.writestr(f'{col}.png', img_buffer.getvalue())
                            if data_index == 0 and index == 0:
                                st.pyplot(fig_box)
                            plt.close(fig_box)
                    zip_buffer.seek(0)
                    st.download_button(
                        label=f"ä¸‹è½½{device_name}ç®±å‹å›¾",
                        data=zip_buffer,
                        file_name=f"{device_name}_ç®±å‹å›¾.zip",
                        mime="application/zip"

                    )

def step_three_data_filter(read_data_list, result_folder):
    # ç¬¬ä¸‰æ­¥ï¼šæ•°æ®è¿‡æ»¤
    st.subheader("æ­¥éª¤3:æ•°æ®æ¸…æ´—")
    # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
    filtered_data_list = []
    if st.checkbox('æ•°æ®æ¸…æ´—', key='æ•°æ®æ¸…æ´—'):
        filter_method = st.radio(
            "è¿‡æ»¤æ–¹æ³•é€‰æ‹©",
            ("ç®±å‹å›¾è¿‡æ»¤", "3-Sigmaè¿‡æ»¤", "ç§»åŠ¨å¹³å‡è¿‡æ»¤"),
        )
        # åº”ç”¨è¿‡æ»¤æ–¹æ³•
        for data_index, data_item in enumerate(read_data_list):
            device_name, df = data_item
            filtered_df = filter_data(df, filter_method)
            filtered_data_list.append((device_name, filtered_df))
            # ä¸éœ€è¦è¿‡æ»¤å’Œèšç±»çš„åˆ—
            date_col = df.filter(like='æ—¶é—´').columns[0]
            device_col = df.filter(like='è®¾å¤‡').columns[0]

            remaining_columns = [col for col in df.columns if col not in [date_col, device_col]]
            exclusive_columns = [col for col in df.columns if 'æ—¶é—´' in col or 'ç¼–ç ' in col]

            # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
            filtered_folder = filter_method.replace(" ", "_").lower()
            filtered_path = os.path.join(result_folder, 'è¿‡æ»¤ç»“æœ', f'{device_name}_è¿‡æ»¤ç»“æœ', filtered_folder)
            os.makedirs(filtered_path, exist_ok=True)
            file_path = os.path.join(filtered_path, f"{device_name}({filtered_folder}ç»“æœ).xlsx")
            if not filtered_df.empty:
                filtered_df.to_excel(file_path, index=False)
                st.success(
                    f"è¿‡æ»¤åçš„æ•°æ®(è¡Œæ•°:{filtered_df.shape[0]},æ•°æ®åˆ—æ•°:{filtered_df.shape[1]}),ç»“æœå·²ä¿å­˜åˆ°ç›®å½•:'{file_path}'")
            else:
                st.warning(f"ç»“æœæœªä¿å­˜åˆ°:{file_path}")
    return filtered_data_list


def step_four_condition_split(read_data_list, filtered_data_list, result_folder, subfolders):
    # ç¬¬å››æ­¥ï¼šå·¥å†µåˆ†å‰²(ä½¿ç”¨èšç±»æ“ä½œ)
    st.subheader("æ­¥éª¤4:å·¥å†µåˆ†å‰²")
    kmeans_col, linkage_col = st.columns(2)
    with kmeans_col:
        if st.checkbox('Kmeansèšç±»', key='kmeans_cluster'):
            cluster_data = filtered_data_list if len(filtered_data_list) != 0 else read_data_list
            for index, item in enumerate(cluster_data):
                device_name, df = item
                df.rename(columns=lambda col: col.replace('"', ''), inplace=True)
                df.rename(columns=lambda col: col.replace('/', ''), inplace=True)
                # ä¸éœ€è¦è¿‡æ»¤å’Œèšç±»çš„åˆ—
                date_col = df.filter(like='æ—¶é—´').columns[0]
                device_col = df.filter(like='è®¾å¤‡').columns[0]

                remaining_columns = [col for col in df.columns if col not in [date_col, device_col]]
                exclusive_columns = [col for col in df.columns if 'æ—¶é—´' in col or 'ç¼–ç ' in col]
                # èšç±»æ‰€éœ€çš„æ•°å€¼å‹æ•°æ®
                if len(remaining_columns) > 0:
                    # é€‰æ‹©èšç±»æ•°ç›®
                    num_clusters = st.slider("é€‰æ‹©kmeansèšç±»æ•°é‡", 2, 6, 2, key=f'{device_name}_èšç±»')

                    # KMeansèšç±»
                    for cluster_index, column in enumerate(remaining_columns):
                        df_cluster_date = df[['ä¿¡å·æ—¶é—´', column]]  # é€‰æ‹©ä¸€åˆ—
                        df_cluster = df[[column]]  # é€‰æ‹©ä¸€åˆ—
                        cluster_data = df_cluster.values.reshape(-1, 1)  # è½¬ä¸ºarray
                        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
                        df_cluster_date['Cluster'] = kmeans.fit_predict(cluster_data)

                        # å¯è§†åŒ–èšç±»ç»“æœ
                        fig_cluster, ax_cluster = plt.subplots()
                        plt.title(f'{column}')
                        plt.xlabel('time')
                        plt.ylabel('variable')
                        sns.scatterplot(data=df_cluster_date,
                                        x=pd.to_datetime(df_cluster_date['ä¿¡å·æ—¶é—´']),
                                        y=f'{column}',
                                        hue='Cluster',
                                        palette='tab10',
                                        ax=ax_cluster
                                        )
                        # å°†èšç±»ç»“æœä¿å­˜åˆ°æ–‡ä»¶
                        clustered_file_path = os.path.join(result_folder, subfolders['cluster_dir'], 'Kmeansèšç±»ç»“æœ',
                                                           f'{device_name}_èšç±»ç»“æœ')
                        os.makedirs(clustered_file_path, exist_ok=True)
                        image_file = os.path.join(clustered_file_path, column + '.png')
                        plt.xticks(rotation=30)
                        plt.legend(loc='best')
                        plt.savefig(image_file, dpi=300)
                        plt.close()

                        if index == 0 and cluster_index == 0:
                            # æ˜¾ç¤ºèšç±»ç»“æœ
                            st.subheader(f'èšç±»ç»“æœ (K={num_clusters})')
                            st.pyplot(fig_cluster)
                else:
                    st.warning("æ²¡æœ‰è¶³å¤Ÿçš„æ•°å€¼å‹æ•°æ®è¿›è¡Œèšç±»åˆ†æã€‚")
            st.success(f'kmeansèšç±»å·²å®Œæˆ!')
    with linkage_col:
        if st.checkbox('å±‚æ¬¡èšç±»', key='linkage_cluster'):
            cluster_data = filtered_data_list if len(filtered_data_list) != 0 else read_data_list
            # æ— ç›‘ç£å­¦ä¹ ï¼Œå‡èšå±‚æ¬¡èšç±»
            for device_name, real_data in cluster_data:
                # å»é™¤åˆ—åçš„ç‰¹æ®Šå­—ç¬¦
                real_data.rename(columns=lambda col: col.replace('"', ''), inplace=True)
                real_data.rename(columns=lambda col: col.replace('/', ''), inplace=True)

                # åˆ é™¤ä¸éœ€è¦èšç±»çš„åˆ—
                if set(['è®¾å¤‡ç¼–ç ', 'Unnamed: 0']).issubset(set(real_data.columns.tolist())):
                    real_data.drop(columns=['è®¾å¤‡ç¼–ç ', 'Unnamed: 0'], inplace=True)

                columns = real_data.columns  # æå–åˆ—å
                value = 'ä¿¡å·æ—¶é—´'  # æ•°æ®ä¿ç•™æ•…æ—¶é—´ï¼Œç”¨äºèšç±»ç»“æœæŒ‰æ—¶é—´ä½œå›¾ï¼Œä½†åˆ—ååˆ é™¤æ•…éšœæ ‡è®°
                device_code = 'è®¾å¤‡ç¼–ç '
                columns = [item for item in columns if item != value and item != device_code]

                # èšç±»
                linkage_cluster_path = os.path.join(result_folder, subfolders['cluster_dir'], 'å±‚æ¬¡èšç±»ç»“æœ',
                                                    device_name + 'èšç±»å›¾')
                os.makedirs(linkage_cluster_path, exist_ok=True)
                fig = plt.figure()
                for column in columns:
                    df_2col = real_data[['ä¿¡å·æ—¶é—´', column]]  # é€‰æ‹©ä¸€åˆ—
                    df_1col = real_data[[column]]  # é€‰æ‹©ä¸€åˆ—
                    real_data_x = df_1col.values.reshape(-1, 1)  # è½¬ä¸ºarray

                    # è®¡ç®—å±‚æ¬¡èšç±»çš„é“¾æ¥çŸ©é˜µ
                    Z = linkage(real_data_x, method='ward')

                    # æ ¹æ®è·ç¦»é˜ˆå€¼æå–ç°‡
                    max_d = 50  # è·ç¦»é˜ˆå€¼
                    clusters = fcluster(Z, max_d, criterion='distance')

                    # å¯è§†åŒ–èšç±»ç»“æœ
                    plt.figure()
                    plt.title(f'{column}')
                    plt.xlabel('time')
                    plt.ylabel('variable')
                    plt.scatter(pd.to_datetime(df_2col['ä¿¡å·æ—¶é—´']),
                                real_data_x[:, 0],
                                c=clusters,
                                cmap='viridis',
                                marker='o',
                                edgecolor='k', s=100)
                    image_file = os.path.join(linkage_cluster_path, column + '.png')
                    plt.xticks(rotation=30)
                    plt.savefig(image_file, dpi=300)
                    plt.close()
            st.success(f'å±‚æ¬¡èšç±»å·²å®Œæˆ!')


@calculate_runtime
def step_five_feature_generate(read_data_list, filtered_data_list, data, result_folder, subfolder):
    # æ­¥éª¤5ï¼šç‰¹å¾ç”Ÿæˆ
    st.subheader("æ­¥éª¤5:ç‰¹å¾ç”Ÿæˆ")
    col1, col2 = st.columns(2)
    with col1:
        if st.checkbox('tsfreshæ–¹æ³•', key='tsfresh'):
            record_result_all = []
            record_result_all2 = []
            record_result_all3 = []

            tsfresh_feature_generate_data = filtered_data_list if len(filtered_data_list) != 0 else read_data_list
            for device_name, df in tsfresh_feature_generate_data:
                # å¯¹æ—¶é—´åˆ—æ ¼å¼åŒ–ä¸ºdatetime
                time_columns = [col for col in data.columns if 'æ—¶é—´' in col]
                for col in time_columns:
                    data[col] = pd.to_datetime(data[col])

                matching_rows = data[data['è®¾å¤‡åç§°'] == device_name]
                equip_info = matching_rows.reset_index(drop=True)  # å¤åˆ¶æ•…éšœæ—¶é—´çš„è®°å½•ï¼Œç”¨äºè¾“å‡ºå…¨éƒ¨ç»“æœ
                if len(matching_rows) > 0:
                    for times in range(len(matching_rows)):
                        trouble_type = matching_rows['æ•…éšœç±»å‹'].iloc[
                            times]  # æ”¹ç”¨æœ¬æ¬¡å¾ªç¯åŒ¹é…åˆ°çš„æ•°æ®ï¼Œè·å–æ•…éšœç±»å‹çš„ä¿¡æ¯
                        stamp_start = matching_rows['è®­ç»ƒé›†å¼€å§‹æ—¶é—´'].iloc[times]
                        stamp_end = matching_rows['è®­ç»ƒé›†ç»“æŸæ—¶é—´'].iloc[times]
                        trouble_time = matching_rows['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'].iloc[times]
                        df['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'] = trouble_time

                        trouble_time_list = []
                        datetime_start = ''
                        datetime_end = ''
                        for times1 in range(len(matching_rows)):
                            if (matching_rows['æ•…éšœç±»å‹'].iloc[times1] == trouble_type):
                                datetime_start = matching_rows['æ•…éšœå¼€å§‹æ—¶é—´'].iloc[times1]
                                datetime_end = matching_rows['æ•…éšœç»“æŸæ—¶é—´'].iloc[times1]
                                trouble_time_list.append(
                                    str(datetime_start) + ',' + str(datetime_end))

                        # excelæ–‡æ¡£è®°å½•
                        extracted_features = pd.DataFrame()
                        features_filtered = pd.DataFrame()
                        record_result = pd.DataFrame()
                        data_nodup = pd.DataFrame()
                        data_nodup_merge = pd.DataFrame()

                        # æˆªå–æ•…éšœæ—¶æ®µçš„æ•°æ®
                        df['ä¿¡å·æ—¶é—´'] = pd.to_datetime(df['ä¿¡å·æ—¶é—´'])
                        new_df = df.loc[
                                 (df.loc[:, 'ä¿¡å·æ—¶é—´'] >= stamp_start) & (
                                         df.loc[:, 'ä¿¡å·æ—¶é—´'] < stamp_end), :]

                        length_new_df1 = len(new_df)

                        # å»é™¤åˆ—åçš„ç‰¹æ®Šå­—ç¬¦
                        new_df.rename(columns=lambda col: col.replace('"', ''), inplace=True)
                        new_df.rename(columns=lambda col: col.replace('/', ''), inplace=True)

                        length2 = 0
                        for times2 in trouble_time_list:
                            datetime_start1 = times2.split(',')[0]
                            datetime_end1 = times2.split(',')[1]
                            length2 += len(
                                new_df.loc[(new_df.loc[:, 'ä¿¡å·æ—¶é—´'] >= datetime_start1) & (
                                        new_df.loc[:, 'ä¿¡å·æ—¶é—´'] < datetime_end1), 'ä¿¡å·æ—¶é—´'])

                        # è¾“å‡ºæ¯æ¬¡æ•…éšœçš„è®­ç»ƒæ•°æ®çš„ä¿¡æ¯
                        record_result['è®¾å¤‡åç§°'] = [equip_info['è®¾å¤‡åç§°'][times]]
                        record_result['æŠ¥å‘Šå‘å‡ºæ—¶é—´'] = [trouble_time]
                        record_result['è®­ç»ƒé›†å¼€å§‹æ—¶é—´'] = [stamp_start]
                        record_result['è®­ç»ƒé›†ç»“æŸæ—¶é—´'] = [stamp_end]
                        record_result['æ•…éšœå¼€å§‹æ—¶é—´'] = [
                            matching_rows['æ•…éšœå¼€å§‹æ—¶é—´'].iloc[times]]
                        record_result['æ•…éšœç»“æŸæ—¶é—´'] = [
                            matching_rows['æ•…éšœç»“æŸæ—¶é—´'].iloc[times]]
                        record_result['è®­ç»ƒé›†å…¨éƒ¨ç‚¹æ•°'] = [length_new_df1]
                        record_result['æ•…éšœæ—¶é—´æ®µç‚¹æ•°'] = [length2]

                        tsfresh_feature_generate_path = os.path.join(result_folder,
                                                                     subfolder['feature_generate_based_on_tsfresh'],
                                                                     device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(trouble_time)[
                                                                                                      :10] + ')' + "tsfreshç‰¹å¾ç”Ÿæˆ.xlsx")
                        if not os.path.exists(tsfresh_feature_generate_path):
                            if length2 > 30:
                                # å°†æ•…éšœæ—¶é—´æ®µè®¾ç½®ä¸º1
                                new_df['æ•…éšœ'] = False
                                datetime_start1 = ''
                                datetime_end1 = ''
                                for times2 in trouble_time_list:
                                    datetime_start1 = times2.split(',')[0]
                                    datetime_end1 = times2.split(',')[1]
                                    new_df.loc[
                                        (new_df.loc[:, 'ä¿¡å·æ—¶é—´'] >= datetime_start1) & (
                                                new_df.loc[:,
                                                'ä¿¡å·æ—¶é—´'] < datetime_end1), 'æ•…éšœ'] = True

                                new_df_f = new_df.copy()  # å¤åˆ¶ä¸€ä¸ªè¡¨æ ¼ï¼Œç”¨äºç‰¹å¾ç”Ÿæˆå’Œç‰¹å¾é€‰æ‹©

                                # é€‰æ‹©éœ€è¦åˆå¹¶çš„åˆ—
                                new_df_f['year'] = new_df_f['ä¿¡å·æ—¶é—´'].dt.year
                                new_df_f['month'] = new_df_f['ä¿¡å·æ—¶é—´'].dt.month  # æŒ‰æœˆç»Ÿè®¡
                                new_df_f['week'] = new_df_f['ä¿¡å·æ—¶é—´'].apply(
                                    lambda x: x.isocalendar()[1])  # æŒ‰å‘¨ç»Ÿè®¡
                                new_df_f['day'] = new_df_f['ä¿¡å·æ—¶é—´'].dt.day  # æŒ‰æ—¥ç»Ÿè®¡
                                cols_to_merge = ['year', 'week',
                                                 'æ•…éšœ']  # æŒ‰æœˆæˆ–æŒ‰å‘¨æˆ–æŒ‰å¤©æˆ–æŒ‰å°æ—¶ç»Ÿè®¡ï¼Œé€‰æ‹©ä¸€ç§ç»Ÿè®¡æ–¹å¼ï¼Œé€‰æ‹©çš„å­—æ®µå†™å…¥è¯¥è¡Œï¼Œç»Ÿè®¡èŒƒå›´å½±å“ç”Ÿæˆçš„ç‰¹å¾å’Œç‰¹å¾é€‰æ‹©çš„ç»“æœ
                                # å®šä¹‰è¿æ¥å­—ç¬¦
                                join_char = '-'
                                # åˆå¹¶åˆ—å¹¶åˆ›å»ºæ–°åˆ—
                                new_df_f['æ•…éšœæ—¶é—´åˆ†ç»„'] = new_df_f[cols_to_merge].astype(
                                    str).agg(
                                    lambda x: join_char.join(x),
                                    axis=1)

                                # è·å–ä¿¡å·æ—¶é—´å’Œæ•…éšœæ ‡è®°ï¼Œç”¨äºä¸ç‰¹å¾é€‰æ‹©ç»“æœçš„åˆå¹¶
                                # æ¯ä¸ªæ•…éšœæ—¶é—´åˆ†ç»„ä¿ç•™ä¸€ä¸ªä¿¡å·æ—¶é—´ã€æ•…éšœæ—¶é—´ï¼Œä¸ç”Ÿæˆçš„ç‰¹å¾åˆå¹¶ï¼Œåšè¶‹åŠ¿å±•ç¤ºæŒ‰æ—¶é—´æ’åºï¼Œä¸”æ˜¾ç¤ºæ•…éšœæ—¶é—´
                                data_nodup = new_df_f.groupby('æ•…éšœæ—¶é—´åˆ†ç»„').head(1)
                                data_nodup_merge = data_nodup[
                                    ['ä¿¡å·æ—¶é—´', 'æ•…éšœæ—¶é—´åˆ†ç»„', 'è®¾å¤‡ç¼–ç ',
                                     'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´']].reset_index()
                                data_nodup_merge.drop(columns='index', inplace=True)

                                # TsFreshæ—¶é—´åºåˆ—çš„ç‰¹å¾å·¥ç¨‹ï¼Œå®˜ç½‘ä»£ç 
                                # é€‰å–æ•…éšœçš„ä¿¡æ¯ä½œä¸ºç‰¹å¾é€‰æ‹©çš„ç›®æ ‡y
                                df_y = new_df_f[
                                    ['æ•…éšœ', 'æ•…éšœæ—¶é—´åˆ†ç»„']].drop_duplicates().set_index(
                                    'æ•…éšœæ—¶é—´åˆ†ç»„')  # æå–ç›®æ ‡yçš„è¡¨æ ¼
                                unique_count_y = len(
                                    set(df_y['æ•…éšœ']))  # yçš„å€¼å¿…é¡»æ˜¯2ä¸ªï¼Œtrueå’Œfalseï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªå€¼ï¼Œä¸åšç‰¹å¾å·¥ç¨‹

                                if unique_count_y > 1:
                                    columns = new_df_f.columns
                                    if 'è®¾å¤‡ç¼–ç ' in columns:
                                        del new_df_f['è®¾å¤‡ç¼–ç ']  # åˆ é™¤ä¸éœ€è¦ç”Ÿæˆç‰¹å¾çš„å‚æ•°
                                    if 'year' in columns:
                                        del new_df_f['year']
                                    if 'month' in columns:
                                        del new_df_f['month']
                                    if 'week' in columns:
                                        del new_df_f['week']
                                    if 'day' in columns:
                                        del new_df_f['day']
                                    if 'æ•…éšœ' in columns:
                                        del new_df_f['æ•…éšœ']
                                    if 'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´' in columns:
                                        del new_df_f['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´']
                                    if 'Unnamed: 0' in columns:
                                        del new_df_f['Unnamed: 0']

                                    extracted_features = extract_features(new_df_f,
                                                                          column_id="æ•…éšœæ—¶é—´åˆ†ç»„",
                                                                          column_sort="ä¿¡å·æ—¶é—´")  # ç”Ÿæˆç‰¹å¾

                                    df_y_sorted = df_y.reindex(
                                        extracted_features.index.tolist())  # ç›®æ ‡yå’Œç”Ÿæˆçš„ç‰¹å¾çš„indexåŒæ­¥

                                    # ç‰¹å¾é€‰æ‹©ï¼Œfdr_levelåº”è¯¥è®¾ç½®ä¸º0.05ï¼Œé¦–å…ˆè®¾ç½®ä¸º0.1ï¼Œå¦‚æœæ²¡æœ‰é€‰å‡ºç¬¦åˆæ¡ä»¶çš„ç‰¹å¾ï¼Œæ”¾å®½é€‰æ‹©æ¡ä»¶ï¼Œ0.1*1.5ï¼Œ
                                    # å¦‚æœé€‰å‡ºè¶…è¿‡100ä¸ªç‰¹å¾ï¼Œé™åˆ¶é€‰æ‹©æ¡ä»¶ï¼Œ0.1*0.5
                                    impute(extracted_features)  # å¡«è¡¥ç¼ºå¤±å€¼

                                    fdr_level_1 = 0.1  # é¦–æ¬¡ç‰¹å¾é€‰æ‹©
                                    features_filtered_1 = select_features(extracted_features,
                                                                          df_y_sorted['æ•…éšœ'],
                                                                          fdr_level=fdr_level_1)

                                    # å¾ªç¯ä¼˜åŒ–ç‰¹å¾é€‰æ‹©
                                    col_count_1 = features_filtered_1.shape[1]

                                    if col_count_1 == 0:
                                        for i in range(1, 16, 1):
                                            # print(i)
                                            fdr_level_i = i / 20.0
                                            features_filtered_i = select_features(
                                                extracted_features,
                                                df_y_sorted['æ•…éšœ'],
                                                fdr_level=fdr_level_i)
                                            col_count_i = features_filtered_i.shape[1]
                                            if col_count_i > 0:
                                                break
                                        features_filtered_final = features_filtered_i
                                        record_result['ç‰¹å¾é€‰æ‹©çš„æ‰¹æ¬¡'] = [i]
                                        record_result['ç‰¹å¾é€‰æ‹©fdri'] = [fdr_level_i]
                                        record_result['ç‰¹å¾é€‰æ‹©çš„ä¸ªæ•°i'] = [col_count_i]
                                    elif col_count_1 > 0 and col_count_1 < 100:
                                        features_filtered_final = features_filtered_1
                                        record_result['ç‰¹å¾é€‰æ‹©çš„æ‰¹æ¬¡'] = [1]
                                        record_result['ç‰¹å¾é€‰æ‹©fdri'] = [fdr_level_1]
                                        record_result['ç‰¹å¾é€‰æ‹©çš„ä¸ªæ•°i'] = [col_count_1]
                                    elif col_count_1 >= 100:
                                        for i in range(1, 20, 1):
                                            # print(i)
                                            fdr_level_i = 0.05 / i
                                            features_filtered_i = select_features(
                                                extracted_features,
                                                df_y_sorted['æ•…éšœ'],
                                                fdr_level=fdr_level_i)
                                            col_count_i = features_filtered_i.shape[1]
                                            if col_count_i == 0:
                                                if i == 1:
                                                    fdr_level_i = fdr_level_1
                                                    features_filtered_i = features_filtered_1
                                                    col_count_i = col_count_1
                                                else:
                                                    fdr_level_i = 0.05 / (i - 1)
                                                    features_filtered_i = select_features(
                                                        extracted_features,
                                                        df_y_sorted['æ•…éšœ'],
                                                        fdr_level=fdr_level_i)
                                                    col_count_i = features_filtered_i.shape[1]
                                                break
                                            elif col_count_1 > 0 and col_count_i < 100:
                                                break
                                        features_filtered_final = features_filtered_i
                                        record_result['ç‰¹å¾é€‰æ‹©çš„æ‰¹æ¬¡'] = [i]
                                        record_result['ç‰¹å¾é€‰æ‹©fdri'] = [fdr_level_i]
                                        record_result['ç‰¹å¾é€‰æ‹©çš„ä¸ªæ•°i'] = [col_count_i]

                                        # è¾“å‡ºç‰¹å¾ç”Ÿæˆå’Œç‰¹å¾é€‰æ‹©çš„ç»“æœ
                                    extracted_features['æ•…éšœæ ‡è®°'] = df_y_sorted[
                                        'æ•…éšœ']  # æ·»åŠ æ•…éšœæ ‡è®°å’Œæ•…éšœæ—¶é—´æ®µçš„åˆ†ç»„
                                    extracted_features['æ•…éšœæ—¶é—´åˆ†ç»„'] = df_y_sorted.index
                                    features_filtered_final['æ•…éšœæ ‡è®°'] = df_y_sorted['æ•…éšœ']
                                    features_filtered_final['æ•…éšœæ—¶é—´åˆ†ç»„'] = df_y_sorted.index
                                    # åˆå¹¶é€‰æ‹©çš„ç‰¹å¾å’Œä¿¡å·æ—¶é—´
                                    extracted_features_merge = pd.merge(extracted_features,
                                                                        data_nodup_merge,
                                                                        how="left",
                                                                        left_on=[
                                                                            "æ•…éšœæ—¶é—´åˆ†ç»„"],
                                                                        right_on=[
                                                                            "æ•…éšœæ—¶é—´åˆ†ç»„"])
                                    extracted_features_merge_nodup = extracted_features_merge.drop_duplicates(
                                        subset='æ•…éšœæ—¶é—´åˆ†ç»„').reset_index()  # å»é™¤åˆå¹¶åçš„é‡å¤è¡Œ
                                    extracted_features_merge_nodup.drop(columns=['index'], inplace=True)

                                    features_filtered_merge = pd.merge(features_filtered_final,
                                                                       data_nodup_merge,
                                                                       how="left",
                                                                       left_on=["æ•…éšœæ—¶é—´åˆ†ç»„"],
                                                                       right_on=[
                                                                           "æ•…éšœæ—¶é—´åˆ†ç»„"])
                                    features_filtered_merge_nodup = features_filtered_merge.drop_duplicates(
                                        subset='æ•…éšœæ—¶é—´åˆ†ç»„').reset_index()  # å»é™¤åˆå¹¶åçš„é‡å¤è¡Œ
                                    features_filtered_merge_nodup.drop(columns=['index'], inplace=True)

                                    # tsfreshç‰¹å¾ç”Ÿæˆ
                                    extracted_features_merge_nodup.to_excel(
                                        os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh'],
                                                     device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                         trouble_time)[
                                                                                      :10] + ')' + "tsfreshç‰¹å¾ç”Ÿæˆ.xlsx"),
                                        index=False)

                                    # tsfreshç‰¹å¾é€‰æ‹©å¾ªç¯ç»†åŒ–
                                    features_filtered_merge_nodup.to_excel(
                                        os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh'],
                                                     device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                         trouble_time)[
                                                                                      :10] + ')' + "tsfreshç‰¹å¾é€‰æ‹©å¾ªç¯ç»†åŒ–.xlsx"),
                                        index=False)

                                    plot_tsfresh_probability_density_image(
                                        os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh']),
                                        device_name,
                                        features_filtered_merge_nodup,
                                        record_result_all2)
                                    plot_tsfresh_trend_scatter_image(
                                        os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh']),
                                        device_name,
                                        features_filtered_merge_nodup,
                                        record_result_all3)
                        record_result_all.append(record_result)

            # æ¯æ¬¡æ•…éšœçš„è®­ç»ƒæ•°æ®çš„ä¿¡æ¯.csv
            if len(record_result_all) != 0:
                pd.concat(record_result_all).to_csv(
                    os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh'],
                                 "æ¯æ¬¡æ•…éšœçš„è®­ç»ƒæ•°æ®çš„ä¿¡æ¯.csv"), encoding='utf-8-sig')

            # ä¸€æ‰¹è®¾å¤‡çš„ç‰¹å¾é€‰æ‹©çš„ç»“æœï¼Œæ‰¹é‡ä½œå›¾ï¼Œæ¦‚ç‡å¯†åº¦å›¾ï¼Œæ•°æ®åŒºåˆ†æ•…éšœå’Œæ­£å¸¸
            if len(record_result_all2) != 0:
                pd.concat(record_result_all2).to_csv(
                    os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh'],
                                 "ç‰¹å¾é€‰æ‹©ä½œå›¾çš„ä¿¡æ¯_å¾ªç¯ä¼˜åŒ–_æ¦‚ç‡å¯†åº¦å›¾.csv"),
                    encoding='utf-8-sig')
            if len(record_result_all3) != 0:
                pd.concat(record_result_all3).to_excel(
                    os.path.join(result_folder, subfolder['feature_generate_based_on_tsfresh'],
                                 "ç‰¹å¾é€‰æ‹©ä½œå›¾çš„ä¿¡æ¯_å¾ªç¯ä¼˜åŒ–_è¶‹åŠ¿å›¾.xlsx"),
                    index=False)

            st.success("tsfreshç‰¹å¾ç”Ÿæˆå·²å®Œæˆ!")

    with col2:
        st.warning('æç¤ºï¼šæ‰§è¡ŒECSMæ–¹æ³•éœ€è¦è€—è´¹è¾ƒé•¿æ—¶é—´!')
        if st.checkbox('ECSM(Exceedance Combination Selection Model)æ–¹æ³•',
                       key='ECSM'):
            ecsm_feature_generate_data = filtered_data_list if len(filtered_data_list) != 0 else read_data_list
            for device_name, df in ecsm_feature_generate_data:
                # ç”Ÿæˆç‰¹å¾ç»„åˆ
                columns = df.columns[4:].tolist()
                # å¦‚æœâ€˜è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´â€™åœ¨åˆ—è¡¨ä¸­ï¼Œåˆ™åˆ é™¤
                if 'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´' in columns:
                    columns.remove('è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´')

                combinations_list = list(combinations(columns, 2))
                length_df = len(df)
                df['ä¿¡å·æ—¶é—´'] = pd.to_datetime(df['ä¿¡å·æ—¶é—´'])

                total_rows = []
                for j in columns:
                    min1 = df.loc[:, j].median() / 5
                    less_than_median_indices = [i for i in df.index if df.loc[i, j] < min1]
                    total_rows = total_rows + less_than_median_indices
                unique_list = list(set(total_rows))

                time_columns = [col for col in data.columns if 'æ—¶é—´' in col]
                for col in time_columns:
                    data[col] = pd.to_datetime(data[col])

                matching_rows = data[data['è®¾å¤‡åç§°'] == device_name]
                # å°†åœ¨æ•…éšœæ—¶é—´æ®µçš„æ•°æ®åºå·æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                fault_time_list = []
                for times in range(len(matching_rows)):
                    # å°†æ•…éšœæ—¶é—´æ®µçš„åºåˆ—å·æå–å‡ºæ¥
                    for i in range(len(df)):
                        if df.loc[i, 'ä¿¡å·æ—¶é—´'] >= matching_rows['æ•…éšœå¼€å§‹æ—¶é—´'].iloc[
                            times] and \
                                df.loc[
                                    i, 'ä¿¡å·æ—¶é—´'] <= \
                                matching_rows['æ•…éšœç»“æŸæ—¶é—´'].iloc[times]:
                            fault_time_list.append(i)

                    # å°†æ•…éšœæ—¶é—´æ®µä»unique_listä¸­åˆ é™¤
                    if len(fault_time_list) < 200 or (
                            len(fault_time_list) < (len(unique_list) / 4) and len(
                        fault_time_list) > 200):
                        for i in fault_time_list:
                            if i in unique_list:
                                unique_list.remove(i)

                df = pd.DataFrame(df.drop(unique_list))

                for combination in combinations_list:
                    column1 = combination[0]
                    column2 = combination[1]
                    if column1[3:] == column2[3:]:
                        # df[column1 + "+" + column2] = df[column1] + df[column2]
                        df[column1 + "-" + column2] = df[column1] - df[column2]
                        df[column1 + " é™¤ " + column2] = df[column1] / df[column2]
                        df[column2 + "-" + column1] = df[column2] - df[column1]
                        df[column2 + " é™¤ " + column1] = df[column2] / df[column1]
                    elif column1[3:] == column2[3:]:
                        # df[column1 + "+" + column2] = df[column1] + df[column2]
                        df[column1 + "-" + column2] = df[column2] - df[column1]
                        df[column1 + " é™¤ " + column2] = df[column1] / df[column2]
                        df[column2 + "-" + column1] = df[column2] - df[column1]
                        df[column2 + " é™¤ " + column1] = df[column2] / df[column1]
                length = len(df.columns)

                # è®¾ç½®é˜ˆå€¼
                length1 = 5
                columns = df.columns[4:].tolist()
                # å¦‚æœâ€˜è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´â€™åœ¨åˆ—è¡¨ä¸­ï¼Œåˆ™åˆ é™¤
                if 'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´' in columns:
                    columns.remove('è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´')
                df.index = range(len(df))
                for column1 in columns:
                    median1 = round(df[column1].quantile(0.5), 2)
                    max1 = round(df[column1].max(), 2)
                    step1 = (max1 - median1) / length1

                    for a_threshold in range(1, length1):
                        a_threshold1 = round(median1 + a_threshold * step1, 2)
                        column1_1 = str(column1 + 'â‰¥' + str(a_threshold1))
                        df[column1_1] = df[column1] >= a_threshold1

                # è®¾ç½®æ•…éšœæ—¶é—´æ®µ
                if len(matching_rows) > 0:
                    for times in range(len(matching_rows)):
                        # datetime_start = matching_rows['å¼€å§‹æ—¶é—´'].iloc[times]
                        # datetime_end = matching_rows['ç»“æŸæ—¶é—´'].iloc[times]
                        stamp_start = matching_rows['è®­ç»ƒé›†å¼€å§‹æ—¶é—´'].iloc[times]
                        stamp_end = matching_rows['è®­ç»ƒé›†ç»“æŸæ—¶é—´'].iloc[times]
                        test_start = matching_rows['æµ‹è¯•é›†å¼€å§‹æ—¶é—´'].iloc[times]
                        test_end = matching_rows['æµ‹è¯•é›†ç»“æŸæ—¶é—´'].iloc[times]
                        trouble_time = matching_rows['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'].iloc[times]
                        trouble_name = matching_rows['æ•…éšœç±»å‹'].iloc[times]

                        trouble_time_list = []
                        datetime_start = ''
                        datetime_end = ''
                        for times1 in range(len(matching_rows)):
                            if matching_rows['æ•…éšœç±»å‹'].iloc[times1] == trouble_name:
                                datetime_start = matching_rows['æ•…éšœå¼€å§‹æ—¶é—´'].iloc[times1]
                                datetime_end = matching_rows['æ•…éšœç»“æŸæ—¶é—´'].iloc[times1]
                                trouble_time_list.append(
                                    str(datetime_start) + ',' + str(datetime_end))

                        # å°†åœ¨stamp_startå’Œstamp_endä¹‹é—´çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåœ¨test_startå’Œtest_endä¹‹é—´çš„æ•°æ®è¿›è¡Œæµ‹è¯•
                        new_df = pd.DataFrame(
                            df.loc[
                            (df.loc[:, 'ä¿¡å·æ—¶é—´'] >= stamp_start) & (
                                    df.loc[:, 'ä¿¡å·æ—¶é—´'] < stamp_end),
                            :])
                        test_df = pd.DataFrame(
                            df.loc[
                            (df.loc[:, 'ä¿¡å·æ—¶é—´'] >= test_start) & (
                                    df.loc[:, 'ä¿¡å·æ—¶é—´'] < test_end),
                            :])

                        if os.path.exists(os.path.join(result_folder, subfolder['feature_generate_based_on_ECSM'],
                                                       device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                           trouble_time)[
                                                                                        :10] + ')' + "ç‰¹å¾ç­›é€‰æŠ¥å‘Š(è®­ç»ƒé›†).xlsx")):
                            data_old = pd.read_excel(
                                os.path.join(result_folder, subfolder['feature_generate_based_on_ECSM'],
                                             device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                 trouble_time)[:10] + ')' + "ç‰¹å¾ç­›é€‰æŠ¥å‘Š(è®­ç»ƒé›†).xlsx"))

                            # åˆ›å»ºä¸€ä¸ªç©ºçš„listå¯¹è±¡
                            combinations_list_finished = []
                            for i in range(len(data_old)):
                                column1 = str(data_old['ç‰¹å¾1'].iloc[i]) + 'â‰¥' + str(
                                    data_old['é˜ˆå€¼1'].iloc[i])
                                column2 = str(data_old['ç‰¹å¾2'].iloc[i]) + 'â‰¥' + str(
                                    data_old['é˜ˆå€¼2'].iloc[i])
                                column3 = str(data_old['ç‰¹å¾3'].iloc[i]) + 'â‰¥' + str(
                                    data_old['é˜ˆå€¼3'].iloc[i])
                                column4 = str(data_old['ç‰¹å¾4'].iloc[i]) + 'â‰¥' + str(
                                    data_old['é˜ˆå€¼4'].iloc[i])
                                tuple1 = (column1, column2, column3, column4)
                                combinations_list_finished.append(tuple1)

                        local1 = 0
                        # excelæ–‡æ¡£è®°å½•
                        data_save = pd.DataFrame(
                            columns=['ç‰¹å¾1', 'é˜ˆå€¼1', 'ç‰¹å¾2', 'é˜ˆå€¼2', 'ç‰¹å¾3', 'é˜ˆå€¼3',
                                     'ç‰¹å¾4',
                                     'é˜ˆå€¼4',
                                     'TN(æ— æ•…éšœï¼Œé¢„æµ‹ä¸ºæ— æ•…éšœ)', 'TP(æ•…éšœï¼Œé¢„æµ‹ä¸ºæ•…éšœ)',
                                     'FN(æ•…éšœï¼Œé¢„æµ‹ä¸ºæ— æ•…éšœ)',
                                     'FP(æ— æ•…éšœï¼Œé¢„æµ‹ä¸ºæ•…éšœ)', 'å‡†ç¡®ç‡((TP+TN)/(TP+FN+FP+TN))',
                                     'ç²¾ç¡®ç‡(TP/(TP+FP))',
                                     'å¬å›ç‡(TP/(TP+FN))', 'æ¼æŠ¥ç‡ï¼ˆFN/(FN+TP)ï¼‰',
                                     'çœŸè´Ÿç‡ï¼ˆTN/(FP+TN)ï¼‰',
                                     'F1 ï¼ˆ(2 * Precision * Recall) / ( Precision + Recall))ï¼‰',
                                     'MCC (TP * TN - FP * FN) / ( ( ( TP + FP) * (TP + FN)*(TN + FP) * (TN + FN) ) **0.5 )'])

                        # ç”Ÿæˆç‰¹å¾ç»„åˆ
                        columns = df.columns[length + 1:-1].tolist()
                        num_combination = 4

                        combinations_list = list(combinations(columns, num_combination))
                        combination = combinations_list[0]

                        # å°†æ•…éšœæ—¶é—´æ®µè®¾ç½®ä¸º1
                        new_df['æ•…éšœ'] = 0
                        datetime_start1 = ''
                        datetime_end1 = ''

                        for times2 in trouble_time_list:
                            datetime_start1 = times2.split(',')[0]
                            datetime_end1 = times2.split(',')[1]
                            new_df.loc[(new_df.loc[:, 'ä¿¡å·æ—¶é—´'] >= datetime_start1) & (
                                    new_df.loc[:, 'ä¿¡å·æ—¶é—´'] < datetime_end1), 'æ•…éšœ'] = 1

                        for combination in combinations_list:
                            column1 = combination[0]
                            column2 = combination[1]
                            column3 = combination[2]
                            column4 = combination[3]
                            column1_1 = column1.split('â‰¥')[0]
                            column2_1 = column2.split('â‰¥')[0]
                            column3_1 = column3.split('â‰¥')[0]
                            column4_1 = column4.split('â‰¥')[0]

                            # å°†å˜é‡æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨
                            variables = [column1_1, column2_1, column3_1, column4_1]
                            # j += 1
                            # æ£€æŸ¥åˆ—è¡¨ä¸­æ˜¯å¦å­˜åœ¨é‡å¤å…ƒç´ 
                            if len(set(variables)) == len(
                                    variables) and 'æ•…éšœ' not in combination and 'é¢„æµ‹ç»“æœ' not in combination:

                                new_df['é¢„æµ‹ç»“æœ'] = (new_df[column1] & new_df[column2]) | (
                                        new_df[column3] & new_df[column4])
                                new_df['é¢„æµ‹ç»“æœ'] = new_df['é¢„æµ‹ç»“æœ'].map({True: 1, False: 0})
                                cm = confusion_matrix(new_df['æ•…éšœ'], new_df['é¢„æµ‹ç»“æœ'])
                                total = cm.sum()
                                TN = cm[0][0]
                                TP = cm[1][1]
                                FN = cm[0][1]
                                FP = cm[1][0]

                                Accuracy = round((TP + TN) / total, 4)
                                Precision = round(TP / (FP + TP), 4)
                                Recall = round(TP / (FN + TP), 4)
                                Miss_rate = round(FN / (FN + TP), 4)
                                Specificity = round(TN / (TN + FP), 4)
                                if (Precision + Recall) == 0:
                                    F1 = 0
                                else:
                                    F1 = round((2 * Precision * Recall) / (Precision + Recall),
                                               4)
                                if (((TP + FP) * (TP + FN) * (TN + FP) * (
                                        TN + FN)) ** 0.5) == 0:
                                    MCC = 0
                                else:
                                    MCC = round((TP * TN - FP * FN) / (
                                            ((TP + FP) * (TP + FN) * (TN + FP) * (
                                                    TN + FN)) ** 0.5),
                                                4)

                                # å°†æ··æ·†çŸ©é˜µæ·»åŠ åˆ° Excel æ–‡æ¡£ä¸­
                                local1 += 1
                                new_row = {'ç‰¹å¾1': column1.split('â‰¥')[0],
                                           'é˜ˆå€¼1': column1.split('â‰¥')[1],
                                           'ç‰¹å¾2': column2.split('â‰¥')[0],
                                           'é˜ˆå€¼2': column2.split('â‰¥')[1],
                                           'ç‰¹å¾3': column3.split('â‰¥')[0],
                                           'é˜ˆå€¼3': column3.split('â‰¥')[1],
                                           'ç‰¹å¾4': column4.split('â‰¥')[0],
                                           'é˜ˆå€¼4': column4.split('â‰¥')[1],
                                           'TN(æ— æ•…éšœï¼Œé¢„æµ‹ä¸ºæ— æ•…éšœ)': TN,
                                           'TP(æ•…éšœï¼Œé¢„æµ‹ä¸ºæ•…éšœ)': TP,
                                           'FN(æ•…éšœï¼Œé¢„æµ‹ä¸ºæ— æ•…éšœ)': FN,
                                           'FP(æ— æ•…éšœï¼Œé¢„æµ‹ä¸ºæ•…éšœ)': FP,
                                           'å‡†ç¡®ç‡((TP+TN)/(TP+FN+FP+TN))': Accuracy,
                                           'ç²¾ç¡®ç‡(TP/(TP+FP))': Precision,
                                           'å¬å›ç‡(TP/(TP+FN))': Recall,
                                           'æ¼æŠ¥ç‡ï¼ˆFN/(FN+TP)ï¼‰': Miss_rate,
                                           'çœŸè´Ÿç‡ï¼ˆTN/(FP+TN)ï¼‰': Specificity,
                                           'F1 ï¼ˆ(2 * Precision * Recall) / ( Precision + Recall))ï¼‰': F1,
                                           'MCC (TP * TN - FP * FN) / ( ( ( TP + FP) * (TP + FN)*(TN + FP) * (TN + FN) ) **0.5 )': MCC}
                                # data_save = data_save.append(new_row, ignore_index=True)
                                data_save = pd.concat([data_save, pd.DataFrame(new_df)], ignore_index=True)

                                gc.collect()
                                if (local1 % 10000) == 1:
                                    # å°†data_saveå’Œdata_oldåˆå¹¶
                                    if 'data_old' in locals():
                                        data_save1 = pd.concat([data_save, data_old],
                                                               ignore_index=True)
                                    else:
                                        data_save1 = data_save
                                    data_save1.to_excel(
                                        os.path.join(result_folder, subfolder['feature_generate_based_on_ECSM'],
                                                     device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                         trouble_time)[
                                                                                      :10] + ')' + "ç‰¹å¾ç­›é€‰æŠ¥å‘Š(è®­ç»ƒé›†).xlsx"),
                                        index=False)
                        # ä¿å­˜ Word æ–‡æ¡£
                        data_save1.to_excel(os.path.join(result_folder, subfolder['feature_generate_based_on_ECSM'],
                                                         device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                             trouble_time)[
                                                                                          :10] + ')' + "ç‰¹å¾ç­›é€‰æŠ¥å‘Š(è®­ç»ƒé›†).xlsx"),
                                            index=False)
                        data1 = data_save1.copy()

                        data1_unique = data1.sort_values(
                            by='F1 ï¼ˆ(2 * Precision * Recall) / ( Precision + Recall))ï¼‰',
                            ascending=False).drop_duplicates(subset=['ç‰¹å¾1'])
                        data1_unique = data1_unique.drop_duplicates(subset=['ç‰¹å¾2'])
                        data1_unique = data1_unique.drop_duplicates(subset=['ç‰¹å¾3'])
                        data1_unique = data1_unique.drop_duplicates(subset=['ç‰¹å¾4'])
                        data1_unique.index = range(len(data1_unique))

                        for i in range(300):
                            column1 = data1_unique['ç‰¹å¾1'][i] + "â‰¥" + str(
                                data1_unique['é˜ˆå€¼1'][i])
                            column2 = data1_unique['ç‰¹å¾2'][i] + "â‰¥" + str(
                                data1_unique['é˜ˆå€¼2'][i])
                            column3 = data1_unique['ç‰¹å¾3'][i] + "â‰¥" + str(
                                data1_unique['é˜ˆå€¼3'][i])
                            column4 = data1_unique['ç‰¹å¾4'][i] + "â‰¥" + str(
                                data1_unique['é˜ˆå€¼4'][i])

                            for times2 in trouble_time_list:
                                datetime_start1 = times2.split(',')[0]
                                datetime_end1 = times2.split(',')[1]
                                test_df.loc[(test_df.loc[:, 'ä¿¡å·æ—¶é—´'] >= datetime_start1) & (
                                        test_df.loc[:, 'ä¿¡å·æ—¶é—´'] < datetime_end1), 'æ•…éšœ'] = 1
                                # print("æ•…éšœæ—¶é—´ï¼š", datetime_start1, datetime_end1)

                            test_df['é¢„æµ‹ç»“æœ'] = ((test_df[column1] >= data1_unique['é˜ˆå€¼1'][
                                i]) & (
                                                           test_df[column2] >=
                                                           data1_unique['é˜ˆå€¼2'][i])) | (
                                                          (test_df[column3] >=
                                                           data1_unique['é˜ˆå€¼3'][
                                                               i]) & (
                                                                  test_df[column4] >=
                                                                  data1_unique['é˜ˆå€¼4'][i]))
                            test_df['é¢„æµ‹ç»“æœ'] = test_df['é¢„æµ‹ç»“æœ'].map({True: 1, False: 0})

                            cm = confusion_matrix(test_df['æ•…éšœ'], test_df['é¢„æµ‹ç»“æœ'])
                            total = cm.sum()
                            TN = cm[0][0]
                            TP = cm[1][1]
                            FN = cm[0][1]
                            FP = cm[1][0]

                            Accuracy = round((TP + TN) / total, 4)
                            Precision = round(TP / (FP + TP), 4)
                            Recall = round(TP / (FN + TP), 4)
                            Miss_rate = round(FN / (FN + TP), 4)
                            Specificity = round(TN / (TN + FP), 4)
                            if (Precision + Recall) == 0:
                                F1 = 0
                            else:
                                F1 = round((2 * Precision * Recall) / (Precision + Recall), 4)
                            if (((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5) == 0:
                                MCC = 0
                            else:
                                MCC = round((TP * TN - FP * FN) / (
                                        ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5),
                                            4)

                            if 'æµ‹è¯•é›†TN' not in data1_unique.columns:
                                data1_unique['æµ‹è¯•é›†TN'] = None
                                data1_unique['æµ‹è¯•é›†TP'] = None
                                data1_unique['æµ‹è¯•é›†FN'] = None
                                data1_unique['æµ‹è¯•é›†FP'] = None
                                data1_unique['æµ‹è¯•é›†å‡†ç¡®ç‡'] = None
                                data1_unique['æµ‹è¯•é›†ç²¾ç¡®ç‡'] = None
                                data1_unique['æµ‹è¯•é›†å¬å›ç‡'] = None
                                data1_unique['æµ‹è¯•é›†æ¼æŠ¥ç‡'] = None
                                data1_unique['æµ‹è¯•é›†çœŸè´Ÿç‡'] = None
                                data1_unique['æµ‹è¯•é›†F1'] = None
                                data1_unique['æµ‹è¯•é›†MCC'] = None
                            data1_unique['æµ‹è¯•é›†TN'][i] = TN
                            data1_unique['æµ‹è¯•é›†TP'][i] = TP
                            data1_unique['æµ‹è¯•é›†FN'][i] = FN
                            data1_unique['æµ‹è¯•é›†FP'][i] = FP
                            data1_unique['æµ‹è¯•é›†å‡†ç¡®ç‡'][i] = Accuracy
                            data1_unique['æµ‹è¯•é›†ç²¾ç¡®ç‡'][i] = Precision
                            data1_unique['æµ‹è¯•é›†å¬å›ç‡'][i] = Recall
                            data1_unique['æµ‹è¯•é›†æ¼æŠ¥ç‡'][i] = Miss_rate
                            data1_unique['æµ‹è¯•é›†çœŸè´Ÿç‡'][i] = Specificity
                            data1_unique['æµ‹è¯•é›†F1'][i] = F1
                            data1_unique['æµ‹è¯•é›†MCC'][i] = MCC

                        data1_unique = data1_unique.iloc[:, :300]
                        data1_unique.to_excel(os.path.join(result_folder, subfolder['feature_generate_based_on_ECSM'],
                                                           device_name + '(æŠ¥å‘Šå‘å‡ºæ—¶é—´ï¼š' + str(
                                                               trouble_time)[
                                                                                            :10] + ')' + "ç‰¹å¾ç­›é€‰æŠ¥å‘Š(è®­ç»ƒé›†+æµ‹è¯•é›†).xlsx"),
                                              index=False)
            st.success("ECSMç‰¹å¾ç”Ÿæˆå·²å®Œæˆ!")


def step_six_feature_selection(result_folder, subfolders):
    # æ­¥éª¤6:ç‰¹å¾é€‰æ‹©
    st.subheader("æ­¥éª¤6:ç‰¹å¾é€‰æ‹©")
    second_uploaded_file = st.file_uploader(
        "# è¯»å…¥æŒ¯åŠ¨ä¿¡å·çš„FFTç‰¹å¾å‚æ•°æˆ–FFTç‰¹å¾å‚æ•°çš„tsfreshç”Ÿæˆçš„ç‰¹å¾", type=["xlsx"],
        key='read_generated_features', accept_multiple_files=False)

    if second_uploaded_file is not None:
        df_generated, df_generated_copy, y = data_read_and_process(second_uploaded_file)
        df_noNA, df_standard, df_MinMax, df_normalize, df_binary = data_preprocessing(df_generated_copy)
        feature_importance_estimate(df_generated_copy, df_noNA, y, result_folder, subfolders)
        feature_selection_col1, feature_selection_col2, feature_selection_col3 = st.columns(3)
        # è¿‡æ»¤æ³•
        with feature_selection_col1:
            if st.checkbox('è¿‡æ»¤æ³•', key='è¿‡æ»¤æ³•'):
                if st.checkbox('filter-æ–¹å·®é€‰æ‹©æ³•', key='filter-æ–¹å·®é€‰æ‹©æ³•'):
                    feature_selection_variance(df_generated_copy, df_noNA, result_folder, subfolders)
                if st.checkbox('filter-å¡æ–¹æ£€éªŒ', key=f'filter-å¡æ–¹æ£€éªŒ'):
                    feature_selection_chi2_test(df_generated_copy, df_MinMax, y, result_folder, subfolders)
                if st.checkbox('filter-ç›¸å…³ç³»æ•°æ³•', key=f'filter-ç›¸å…³ç³»æ•°æ³•'):
                    feature_selection_correlation_coefficient(df_generated_copy, y, result_folder, subfolders)
        # åŒ…è£…æ³•
        with feature_selection_col2:
            if st.checkbox('åŒ…è£…æ³•', key=f'åŒ…è£…æ³•'):
                if st.checkbox('wrapper - RFEé€’å½’ç‰¹å¾æ¶ˆé™¤æ³•',
                               key=f'wrapper - RFEé€’å½’ç‰¹å¾æ¶ˆé™¤æ³•'):
                    feature_selection_RFE(df_generated_copy, df_noNA, y, result_folder, subfolders)
        # åµŒå…¥æ³•
        with feature_selection_col3:
            if st.checkbox('åµŒå…¥æ³•', key=f'åµŒå…¥æ³•'):
                if st.checkbox('embedded - åŸºäºL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•',
                               key=f'embedded - åŸºäºL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•'):
                    feature_selection_embedded_based_on_L1(df_generated_copy, df_noNA, y, result_folder, subfolders)
                if st.checkbox('embedded - ç»“åˆL1å’ŒL2æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•',
                               key=f'embedded - ç»“åˆL1å’ŒL2æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•'):
                    feature_selection_embedded_based_on_L1L2(df_generated_copy, df_noNA, y, result_folder,
                                                             subfolders)
                if st.checkbox('embedded - åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•',
                               key=f'embedded - åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•'):
                    feature_selection_embedded_based_on_GBDT(df_generated_copy, df_noNA, y, result_folder,
                                                             subfolders)
                if st.checkbox('ç»“åˆSVMå’ŒL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©',
                               key=f'ç»“åˆSVMå’ŒL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©'):
                    feature_selection_embedded_based_on_SVM_L1(df_generated_copy, df_noNA, y, result_folder,
                                                               subfolders)
                if st.checkbox('LASSO', key=f'ç»“åˆLASSOçš„ç‰¹å¾é€‰æ‹©'):
                    feature_selection_embedded_based_on_Lasso(df_generated_copy, y, result_folder, subfolders)


def plot_tsfresh_probability_density_image(data_folder,
                                           device_name,
                                           features_filtered_merge_nodup,
                                           record_result_all2):
    """
    ç”»tsfreshçš„æ¦‚ç‡å¯†åº¦å›¾
    :return:
    """
    df = features_filtered_merge_nodup.copy()
    col_count = df.shape[1]  # ç‰¹å¾é€‰æ‹©çš„ç»“æœçš„åˆ—æ•°=2ï¼Œè¡¨ç¤ºé€‰æ‹©ç»“æœä¸ºç©ºï¼Œä¸ç”»å›¾ï¼Œ
    # excelæ–‡æ¡£è®°å½•
    record_result = pd.DataFrame()
    record_result['è®¾å¤‡åç§°'] = [device_name]
    record_result['ç‰¹å¾é€‰æ‹©çš„ä¸ªæ•°'] = [col_count]
    record_result_all2.append(record_result)
    if col_count > 5:
        # æ¦‚ç‡å¯†åº¦å›¾
        probability_density_path = os.path.join(data_folder, device_name + 'æ¦‚ç‡å¯†åº¦å›¾')
        os.makedirs(probability_density_path, exist_ok=True)
        fig = plt.figure(figsize=(30, 20))

        # å»é™¤åˆ—åçš„ç‰¹æ®Šå­—ç¬¦
        df.rename(columns=lambda col: col.replace('"', ''), inplace=True)
        df.rename(columns=lambda col: col.replace('/', ''), inplace=True)
        df.drop(columns=['æ•…éšœæ—¶é—´åˆ†ç»„', 'ä¿¡å·æ—¶é—´', 'è®¾å¤‡ç¼–ç ', 'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'], inplace=True)
        columns = df.columns  # æå–åˆ—å
        value = 'æ•…éšœæ ‡è®°'
        columns = [item for item in columns if item != value]

        for column in columns:
            df_1col = df[['æ•…éšœæ ‡è®°', column]]

            df_data1 = df_1col[df_1col['æ•…éšœæ ‡è®°'] == True]
            df_data2 = df_1col[df_1col['æ•…éšœæ ‡è®°'] == False]

            data1 = df_data1[column]
            data2 = df_data2[column]

            plt.figure(figsize=(20, 12))

            sns.kdeplot(data1, fill=True, color='blue', alpha=0.5, label='æ•…éšœ',
                        linewidth=2)
            sns.kdeplot(data2, fill=True, color='orange', alpha=0.5,
                        label='æ­£å¸¸',
                        linewidth=2)

            plt.legend(fontsize=20)
            plt.tick_params(axis='both', labelsize=20)
            plt.title(f'{column}', fontsize=20)
            plt.xlabel('Value', fontsize=20)
            plt.ylabel('Density', fontsize=20)
            plt.xticks(rotation=30)
            image_file = os.path.join(data_folder,
                                      device_name + 'æ¦‚ç‡å¯†åº¦å›¾',
                                      column + '.png')
            plt.savefig(image_file, dpi=300)
            plt.close()


def plot_tsfresh_trend_scatter_image(data_folder,
                                     device_name,
                                     features_filtered_merge_nodup,
                                     record_result_all3):
    """
    ä¸€æ‰¹è®¾å¤‡çš„ç‰¹å¾é€‰æ‹©çš„ç»“æœï¼Œæ‰¹é‡ä½œå›¾ï¼Œæ•£ç‚¹å›¾ï¼Œè¶‹åŠ¿å›¾
    :param data_folder:
    :param device_name:
    :return:
    """
    df = features_filtered_merge_nodup.copy()
    df['ä¿¡å·æ—¶é—´'] = pd.to_datetime(df['ä¿¡å·æ—¶é—´'])  # è®¾ç½®ä¿¡å·æ—¶é—´çš„æ ¼å¼å¹¶æ’åº
    df = df.sort_values('ä¿¡å·æ—¶é—´', ascending=True)
    trouble_time = df['è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'][0]  # è·å–è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´
    col_count = df.shape[1]  # ç‰¹å¾é€‰æ‹©çš„ç»“æœçš„åˆ—æ•°=2ï¼Œè¡¨ç¤ºé€‰æ‹©ç»“æœä¸ºç©ºï¼Œä¸ç”»å›¾ï¼Œ

    # excelæ–‡æ¡£è®°å½•
    record_result = pd.DataFrame()
    record_result['è®¾å¤‡ç¼–ç '] = df['è®¾å¤‡ç¼–ç '][0]
    record_result['ç‰¹å¾é€‰æ‹©çš„ä¸ªæ•°'] = [col_count]
    record_result_all3.append(record_result)

    if col_count > 5:
        # åˆ›å»ºæ–‡ä»¶å¤¹
        tsfresh_trend_path = os.path.join(data_folder, device_name + 'è¶‹åŠ¿å›¾')
        os.makedirs(tsfresh_trend_path, exist_ok=True)
        fig = plt.figure(figsize=(30, 20))

        # å»é™¤åˆ—åçš„ç‰¹æ®Šå­—ç¬¦
        df.rename(columns=lambda col: col.replace('"', ''), inplace=True)
        df.rename(columns=lambda col: col.replace('/', ''), inplace=True)
        df.drop(columns=['æ•…éšœæ—¶é—´åˆ†ç»„', 'æ•…éšœæ ‡è®°', 'è®¾å¤‡ç¼–ç ',
                         'è¯Šæ–­æŠ¥å‘Šå‘å‡ºçš„æ—¶é—´'], inplace=True)

        columns = df.columns  # æå–åˆ—å
        value = 'ä¿¡å·æ—¶é—´'
        columns = [item for item in columns if item != value]

        for column in columns:
            plt.figure(figsize=(20, 12))
            max1 = df[column].max()

            plt.scatter(pd.to_datetime(df['ä¿¡å·æ—¶é—´']), df[column],
                        cmap='viridis',
                        alpha=0.7)
            plt.plot(pd.to_datetime(df['ä¿¡å·æ—¶é—´']), df[column])
            plt.axvline(trouble_time, color='red', linestyle='--',
                        label='æŠ¥å‘Šå‘å‡ºæ—¶é—´')
            plt.text(trouble_time, max1, str(trouble_time)[:10], rotation=0,
                     verticalalignment='center',
                     horizontalalignment='center', fontsize=30)

            plt.tick_params(axis='both', labelsize=20)
            plt.title(f'{column}', fontsize=20)
            plt.xlabel('time', fontsize=20)
            plt.ylabel('value', fontsize=20)
            plt.xticks(rotation=30)
            image_file = os.path.join(tsfresh_trend_path, column + '.png')
            plt.savefig(image_file, dpi=300)
            plt.close()


def main():
    # ä¾§è¾¹æ -ç‰¹å¾å·¥ç¨‹6å¤§æ­¥éª¤
    create_sidebar()

    # æ­¥éª¤1: ä¸Šä¼ æ–‡ä»¶
    read_result = step_one_data_uploader()

    read_data_list, data = read_result  # read_data_listæ˜¯_æ ¼å¼åŒ–ç‰¹å¾é›†ï¼Œdataæ˜¯äººå·¥æ ‡è®°æ•…éšœæ—¶é—´æ•°æ®

    if len(read_data_list) != 0 and data is not None:
        # è¯»å–å…¨éƒ¨åç¼€"_æ ¼å¼åŒ–ç‰¹å¾é›†.csv"çš„æ–‡ä»¶
        # å¦‚æœå¾ªç¯è¯»å–ï¼Œåœ¨st.checkbox('è¶‹åŠ¿å›¾',key=f'è¶‹åŠ¿å›¾')æ—¶ä¼šæŠ¥é”™ï¼Œå› ä¸ºå‚æ•°keyè¦æ±‚å”¯ä¸€
        # åªå±•ç¤ºè¯»å–çš„ç¬¬ä¸€ä¸ªæ•°æ®åŸºæœ¬ä¿¡æ¯
        device_name = read_data_list[0][0]
        df = read_data_list[0][1]  # dfè¡¨ç¤ºè¯»å–çš„ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶
        data_row = df.shape[0]
        data_col = df.shape[1]
        st.write(f"{device_name}_æ•°æ®é¢„è§ˆ(æ•°æ®è¡Œæ•°:{data_row},æ•°æ®åˆ—æ•°:{data_col}):", df.head())  # æ˜¾ç¤ºæ•°æ®çš„å‰5è¡Œ

        # åˆ›å»ºä¿å­˜å…¨éƒ¨ç»“æœçš„æ€»æ–‡ä»¶å¤¹result
        result_folder = './result'
        # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹
        os.makedirs(result_folder, exist_ok=True)

        # å®šä¹‰å„ä¸ªå­æ–‡ä»¶å¤¹è·¯å¾„
        subfolders = {
            'trend_dir': 'è¶‹åŠ¿å›¾',
            'hist_dir': 'ç›´æ–¹å›¾',
            'scatter_dir': 'æ•£ç‚¹å›¾',
            'box_dir': 'ç®±å‹å›¾',
            'cluster_dir': 'èšç±»ç»“æœ',
            'feature_generate_based_on_tsfresh': 'ç‰¹å¾ç”Ÿæˆç»“æœ/tsfresh',
            'feature_generate_based_on_ECSM': 'ç‰¹å¾ç”Ÿæˆç»“æœ/ECSM',
            'filter_variance_selection_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/è¿‡æ»¤æ³•-æ–¹å·®é€‰æ‹©æ³•',
            'filter_chi2_estimator_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/è¿‡æ»¤æ³•-å¡æ–¹æ£€éªŒ',
            'filter_correlation_coefficient_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/è¿‡æ»¤æ³•-ç›¸å…³ç³»æ•°',
            'filter_RFE_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åŒ…è£…æ³•-RFEé€’å½’æ¶ˆé™¤æ³•',
            'filter_based_on_L1_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åµŒå…¥æ³•-åŸºäºL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•',
            'filter_based_on_L1L2_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åµŒå…¥æ³•-ç»“åˆL1å’ŒL2æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©æ³•',
            'filter_base_on_GBDT_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åµŒå…¥æ³•-åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é€‰æ‹©æ³•',
            'filter_based_on_SVM_L1_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åµŒå…¥æ³•-ç»“åˆSVMå’ŒL1æƒ©ç½šé¡¹çš„ç‰¹å¾é€‰æ‹©',
            'filter_based_on_Lasso_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/åµŒå…¥æ³•-LASSO',
            'feature_importance_folder': 'ç‰¹å¾é€‰æ‹©ç»“æœ/ç‰¹å¾é‡è¦æ€§'
        }

        # å¾ªç¯åˆ›å»ºå­æ–‡ä»¶å¤¹
        for folder_name, folder_path in subfolders.items():
            full_path = os.path.join(result_folder, folder_path)
            if folder_path not in ['è¶‹åŠ¿å›¾', 'æ•£ç‚¹å›¾', 'ç›´æ–¹å›¾', 'ç®±å‹å›¾']:
                os.makedirs(full_path, exist_ok=True)

        # æ­¥éª¤2:æ•°æ®å¯è§†åŒ–
        step_two_data_visualization(read_data_list, result_folder, subfolders)

        # æ­¥éª¤3ï¼šæ•°æ®æ¸…æ´—
        filtered_data_list = step_three_data_filter(read_data_list, result_folder)

        # æ­¥éª¤4:å·¥å†µåˆ†å‰²
        step_four_condition_split(read_data_list, filtered_data_list, result_folder, subfolders)

        # æ­¥éª¤5:ç‰¹å¾ç”Ÿæˆ
        step_five_feature_generate(read_data_list, filtered_data_list, data, result_folder, subfolders)

        # æ­¥éª¤6:ç‰¹å¾é€‰æ‹©
        step_six_feature_selection(result_folder, subfolders)


if __name__ == '__main__':
    main()
